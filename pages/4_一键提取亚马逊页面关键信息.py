import streamlit as st
from bs4 import BeautifulSoup
import re
import pandas as pd  # å¯¼å…¥ pandas ç”¨äºè¡¨æ ¼
import requests  # (æ–°å¢) ç”¨äºä¸‹è½½å›¾ç‰‡
import io  # (æ–°å¢) ç”¨äºå†…å­˜ä¸­åˆ›å»ºzip
import zipfile  # (æ–°å¢) ç”¨äºåˆ›å»ºzip


# --- æ‚¨æä¾›çš„åŸå§‹è§£æå‡½æ•° ---
# (ä¸ºäº†ä¿æŒä»£ç æ•´æ´ï¼Œè¿™é‡Œçœç•¥äº†å‡½æ•°çš„å†…éƒ¨å®ç°ï¼Œä½†å®ƒä»¬ä¸æ‚¨æä¾›çš„ä¸€è‡´)

def extract_all_product_info(html_content):
    """
    ä»ç½‘é¡µæºä»£ç ä¸­æå–å•†å“çš„æ‰€æœ‰å…³é”®ä¿¡æ¯
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    results = {}

    # 1. æå–å•†å“ç‰¹æ€§åˆ—è¡¨
    features = extract_features(soup)
    results['features'] = features

    # 2. æå–å•†å“æ ‡é¢˜
    title = extract_title(soup)
    results['title'] = title

    # 3. æå–ä»·æ ¼ä¿¡æ¯
    price = extract_price(soup)
    results['price'] = price

    # 4. æå–è¯„åˆ†ä¿¡æ¯
    rating_info = extract_rating(soup)
    results['rating'] = rating_info

    # 5. æå–æ‰€æœ‰ç”¨æˆ·è¯„è®º (æ–°å¢)
    reviews = extract_reviews(soup)
    results['reviews'] = reviews

    # 6. æå–å•†å“è¯¦ç»†ä¿¡æ¯ (æ–°å¢)
    product_details = extract_product_details(soup)
    results['product_details'] = product_details

    return results


def extract_features(soup):
    """
    æå–å•†å“ç‰¹æ€§åˆ—è¡¨
    """
    features = []
    feature_section = soup.find('div', id='feature-bullets')

    if feature_section:
        list_items = feature_section.find_all('li', class_='a-spacing-mini')
        for item in list_items:
            span = item.find('span', class_='a-list-item')
            if span:
                text = span.get_text(strip=True)
                features.append(text)

    return features


def extract_title(soup):
    """
    æå–å•†å“æ ‡é¢˜
    """
    title_span = soup.find('span', id='productTitle')
    if title_span:
        return title_span.get_text(strip=True)
    return None


def extract_price(soup):
    """
    æå–ä»·æ ¼ä¿¡æ¯
    """
    # æ–¹æ³•1ï¼šé€šè¿‡ç‰¹å®šclasså®šä½
    # å°è¯•æŸ¥æ‰¾ 'a-price' ç»“æ„
    price_span = soup.find('span', class_='a-price')
    if price_span:
        price_text = price_span.find('span', class_='a-offscreen')
        if price_text:
            return price_text.get_text(strip=True).replace('$', '').replace('ï¿¥', '')  # ç§»é™¤è´§å¸ç¬¦å·

    # æ–¹æ³•2ï¼šç»„åˆ 'a-price-symbol', 'a-price-whole', 'a-price-fraction'
    price_symbol = soup.find('span', class_='a-price-symbol')
    if price_symbol:
        whole_price = soup.find('span', class_='a-price-whole')
        fraction_price = soup.find('span', class_='a-price-fraction')

        if whole_price and fraction_price:
            whole_text = whole_price.get_text(strip=True).replace('.', '').replace(',', '')
            fraction_text = fraction_price.get_text(strip=True)
            return f"{whole_text}.{fraction_text}"

    # æ–¹æ³•3ï¼šå¤‡é€‰æ–¹æ¡ˆï¼Œæœç´¢ä»·æ ¼æ¨¡å¼ (ä½œä¸ºæœ€åçš„å°è¯•)
    price_pattern = r'[\$ï¿¥]\d+[\.,]\d{2}'  # åŒ¹é… $19.99 æˆ– ï¿¥19,99
    text_content = soup.get_text()
    price_matches = re.findall(price_pattern, text_content)
    if price_matches:
        # æ¸…ç†æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
        return price_matches[0].replace('$', '').replace('ï¿¥', '').replace(',', '.')

    return None


def extract_rating(soup):
    """
    æå–è¯„åˆ†ä¿¡æ¯
    """
    rating_info = {}

    # æå–è¯„åˆ†æè¿°ï¼ˆä¾‹å¦‚ "4.3 out of 5 stars"ï¼‰
    rating_alt = soup.find('span', class_='a-icon-alt')
    if rating_alt:
        rating_text = rating_alt.get_text(strip=True)
        rating_info['full_text'] = rating_text
        # å°è¯•ä»ä¸­æå–åˆ†æ•°
        score_match = re.search(r'(\d+\.?\d*)\s*out\s*of\s*5', rating_text)
        if score_match:
            rating_info['score'] = score_match.group(1)

    # å¦‚æœä¸Šè¿°æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾è¯„åˆ†æ•°å­—ï¼ˆä¾‹å¦‚ "4.3"ï¼‰
    if 'score' not in rating_info:
        rating_span = soup.find('span', class_='a-size-base', string=re.compile(r'^\d+\.\d$'))
        if rating_span:
            rating_info['score'] = rating_span.get_text(strip=True)

    # æå–è¯„ä»·æ•°é‡
    review_count = soup.find('span', id='acrCustomerReviewText')
    if review_count:
        rating_info['review_count'] = review_count.get_text(strip=True)

    return rating_info


# --- (æ–°å¢) æå–è¯„è®ºå‡½æ•° ---

def extract_reviews(soup):
    """
    æå–æ‰€æœ‰ç”¨æˆ·è¯„è®ºä¿¡æ¯ (å·²é›†æˆ)
    """
    reviews = []

    # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°æ‰€æœ‰è¯„è®ºå®¹å™¨
    # (å·²å¢å¼ºï¼šæ·»åŠ å¤‡ç”¨é€‰æ‹©å™¨)
    review_containers = soup.find_all('div', id=re.compile(r'(customer_review-|review-)'))

    if not review_containers:
        review_containers = soup.find_all('div', attrs={'data-hook': 'review'})

    for container in review_containers:
        review = {}

        # 1. æå–ç”¨æˆ·å
        profile_name = container.find('span', class_='a-profile-name')
        if profile_name:
            review['username'] = profile_name.get_text(strip=True)

        # 2. æå–è¯„è®ºæ ‡é¢˜/æ¦‚è¦
        review_title = container.find('span', class_='review-title-content')
        if review_title:
            review['title'] = review_title.get_text(strip=True)
        else:
            # å¤‡é€‰æ–¹æ³•ï¼šé€šè¿‡data-hookå±æ€§
            title_element = container.find('span', attrs={'data-hook': 'review-title'})
            if title_element:
                # è·å–æ ‡é¢˜æ–‡æœ¬ï¼Œè·³è¿‡æ˜Ÿçº§éƒ¨åˆ†
                title_text = title_element.get_text(strip=True)
                # ç§»é™¤æ˜Ÿçº§æ–‡æœ¬ï¼Œåªä¿ç•™æ ‡é¢˜
                # (è¿™ä¸ªé€»è¾‘å¯èƒ½ä¸æ€»æ˜¯å‡†ç¡®ï¼Œä½†ä¿ç•™åŸæ ·)
                if 'out of 5 stars' in title_text:
                    # å°è¯•æ›´å‡†ç¡®åœ°åˆ†å‰²
                    parts = re.split(r'\d+\.\d out of 5 stars', title_text)
                    if len(parts) > 1:
                        title_text = parts[1].strip()
                    else:
                        # å¤‡ç”¨æ–¹æ¡ˆ
                        title_text = title_text.split('out of 5 stars')[-1].strip()
                review['title'] = title_text

        # 3. æå–æ˜Ÿçº§è¯„åˆ†
        star_rating_icon = container.find('i', attrs={'data-hook': 'review-star-rating'})
        if star_rating_icon:
            # å°è¯•ä» 'a-icon-alt' ä¸­è·å–
            star_text = star_rating_icon.find('span', class_='a-icon-alt')
            if star_text:
                match = re.search(r'(\d+\.?\d*) out of 5 stars', star_text.get_text())
                if match:
                    review['rating'] = match.group(1)

        # å¤‡é€‰æ–¹æ¡ˆ (æ¥è‡ªæ‚¨çš„ä»£ç )
        if 'rating' not in review:
            star_rating = container.find('i', class_=re.compile(r'a-star-\d'))
            if star_rating:
                # ä»classåä¸­æå–æ˜Ÿçº§æ•°å­—
                class_names = star_rating.get('class', [])
                for class_name in class_names:
                    if class_name.startswith('a-star-'):
                        # æå–æ•°å­—ï¼Œå¦‚ "a-star-4" -> "4"
                        review['rating'] = class_name.split('-')[-1].replace('-', '.')  # å…¼å®¹ a-star-4-5
                        break

        # 4. æå–è¯„è®ºæ—¥æœŸ
        review_date = container.find('span', attrs={'data-hook': 'review-date'})
        if review_date:
            review['date'] = review_date.get_text(strip=True)

        # 5. æå–è¯„è®ºæ­£æ–‡
        review_body = container.find('span', attrs={'data-hook': 'review-body'})
        if review_body:
            # è·å–è¯„è®ºæ–‡æœ¬ï¼ŒåŒ…æ‹¬å¯èƒ½è¢«æŠ˜å çš„å†…å®¹
            review_text = review_body.get_text(strip=True)
            review['content'] = review_text

        # 6. æå–æ˜¯å¦æœ‰Verified Purchase
        verified_badge = container.find('span', attrs={'data-hook': 'avp-badge-linkless'})
        if verified_badge and 'Verified Purchase' in verified_badge.get_text():
            review['verified'] = True
        else:
            # å¤‡é€‰
            verified_badge_alt = container.find('span', class_='a-size-mini', string=re.compile('Verified Purchase'))
            if verified_badge_alt:
                review['verified'] = True
            else:
                review['verified'] = False

        # 7. æå–æœ‰å¸®åŠ©çš„æ•°é‡
        helpful_text = container.find('span', attrs={'data-hook': 'helpful-vote-statement'})
        if helpful_text:
            helpful_count = helpful_text.get_text(strip=True)
            review['helpful_count'] = helpful_count

        # 8. æå–å›¾ç‰‡é“¾æ¥ (æ–°å¢)
        image_urls = []
        image_tags = container.find_all('img', attrs={'data-hook': 'review-image-tile'})
        for img in image_tags:
            # ä¼˜å…ˆä½¿ç”¨ data-src (æ‡’åŠ è½½)
            url = img.get('data-src') or img.get('src')
            if url:
                # å°è¯•ç§»é™¤ç¼©ç•¥å›¾åç¼€ (ä¾‹å¦‚ ._SY88.jpg) æ¥è·å–é«˜æ¸…å›¾
                # \._[A-Z0-9_]+_\.jpg$ åŒ¹é…åƒ ._SY88.jpg æˆ– ._AC_UF100_..._.jpg è¿™æ ·çš„åç¼€
                cleaned_url = re.sub(r'\._[A-Z0-9_]+_\.jpg$', '.jpg', url)
                image_urls.append(cleaned_url)

        review['images'] = image_urls

        if review:  # åªæ·»åŠ æœ‰å†…å®¹çš„è¯„è®º
            reviews.append(review)

    return reviews


# --- (æ–°å¢) æå–å•†å“è¯¦ç»†ä¿¡æ¯å‡½æ•° ---
def extract_product_details(soup):
    """
    æå– "Product details" éƒ¨åˆ†çš„ä¿¡æ¯ (ä¾‹å¦‚ ASIN, åˆ¶é€ å•†ç­‰)
    (å·²æ›´æ–°ï¼Œå¯å¤„ç† div å’Œ table ä¸¤ç§ç»“æ„)
    """
    details = {}
    # æŸ¥æ‰¾ "Product Details" å®¹å™¨ (div > ul > li ç»“æ„)
    details_div = soup.find('div', id='detailBullets_feature_div')

    if details_div:
        list_items = details_div.find_all('li')
        for item in list_items:
            key_element = item.find('span', class_='a-text-bold')

            if key_element:
                # 1. è·å–é”® (Key)
                key_text_full = key_element.get_text(strip=True)  # e.g., "ASIN:"
                key_clean = key_text_full.replace(':', '').strip()  # e.g., "ASIN"

                # 2. è·å–å€¼ (Value)
                # ä½¿ç”¨ separator=' ' æ¥æ­£ç¡®å¤„ç† "Best Sellers Rank" ä¸­çš„åµŒå¥—åˆ—è¡¨
                full_item_text = item.get_text(separator=' ', strip=True)
                # æ›¿æ¢æ‰é”®çš„å…¨åï¼Œç„¶åå»é™¤å‰å¯¼çš„å†’å·å’Œç©ºæ ¼
                value_text = full_item_text.replace(key_text_full, '', 1).lstrip(' :').strip()

                # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
                value_text = re.sub(r'\s+', ' ', value_text)

                if value_text and key_clean:
                    # (æ–°å¢) ç‰¹åˆ«æ¸…ç† "Best Sellers Rank"
                    if key_clean == 'Best Sellers Rank':
                        value_text = value_text.split('(See Top 100')[0].strip()
                    details[key_clean] = value_text
    else:
        # å¤‡é€‰æ–¹æ¡ˆ: æŸ¥æ‰¾ (table > tr > th/td ç»“æ„)
        details_table = soup.find('table', id='productDetails_detailBullets_sections1')
        if details_table:
            rows = details_table.find_all('tr')
            for row in rows:
                key_element = row.find('th')
                value_element = row.find('td')
                if key_element and value_element:
                    key_clean = key_element.get_text(strip=True)

                    # (æ–°å¢) è·³è¿‡ "Customer Reviews"ï¼Œå› ä¸ºå®ƒç”± extract_rating å•ç‹¬å¤„ç†
                    if key_clean == 'Customer Reviews':
                        continue

                    value_text_raw = value_element.get_text(separator=' ', strip=True)
                    value_text = re.sub(r'\s+', ' ', value_text_raw).strip()

                    # (æ–°å¢) ç‰¹åˆ«æ¸…ç† "Best Sellers Rank"
                    if key_clean == 'Best Sellers Rank':
                        value_text = value_text.split('(See Top 100')[0].strip()

                    details[key_clean] = value_text

    return details


# --- (æ–°å¢) Streamlit ç¼“å­˜å‡½æ•° ---
@st.cache_data
def convert_df_to_csv(df):
    """
    å°† DataFrame è½¬æ¢ä¸º CSV æ ¼å¼ (ç”¨äºä¸‹è½½)
    ä½¿ç”¨ utf-8-sig ç¼–ç ç¡®ä¿ Excel æ­£ç¡®æ‰“å¼€
    """
    return df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')


# --- (æ–°å¢) ä¸‹è½½å¹¶å‹ç¼©å›¾ç‰‡çš„å‡½æ•° ---
@st.cache_data  # ç¼“å­˜ä¸‹è½½ç»“æœ
def download_and_zip_images(image_urls):
    """
    Downloads images from a list of URLs and returns them in a zipped BytesIO buffer.
    """
    zip_buffer = io.BytesIO()
    # ä½¿ç”¨ with è¯­å¥ç¡®ä¿ zip æ–‡ä»¶è¢«æ­£ç¡®å…³é—­
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        progress_bar = st.progress(0, text="å¼€å§‹ä¸‹è½½...")
        total_images = len(image_urls)

        for i, url in enumerate(image_urls):
            try:
                response = requests.get(url, timeout=10)  # 10 ç§’è¶…æ—¶
                if response.status_code == 200:
                    # ä»URLè·å–ä¸€ä¸ªåŸºæœ¬çš„æ–‡ä»¶å
                    filename = url.split('/')[-1].split('?')[0]
                    # ç¡®ä¿æ–‡ä»¶åæœ‰æ„ä¹‰ï¼Œå¦‚æœå¤ªçŸ­æˆ–æ²¡æœ‰æ‰©å±•å
                    if not filename or not '.' in filename[-5:]:
                        filename = f"image_{i + 1}.jpg"

                    zip_file.writestr(filename, response.content)
                else:
                    st.warning(f"æ— æ³•ä¸‹è½½å›¾ç‰‡ (çŠ¶æ€ç  {response.status_code}): {url}", icon="âš ï¸")
            except requests.RequestException as e:
                st.error(f"ä¸‹è½½å›¾ç‰‡æ—¶å‡ºé”™: {url} (é”™è¯¯: {e})", icon="ğŸ”¥")

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.progress((i + 1) / total_images, text=f"æ­£åœ¨ä¸‹è½½ç¬¬ {i + 1}/{total_images} å¼ å›¾ç‰‡...")

        progress_bar.progress(1.0, text="å‹ç¼©å®Œæˆï¼")

    zip_buffer.seek(0)
    return zip_buffer


# --- (æ–°å¢) æ ¼å¼åŒ–åŸºæœ¬ä¿¡æ¯ ---
def format_other_info_for_display(results):
    """
    å°†åŸºæœ¬ä¿¡æ¯æ ¼å¼åŒ–ä¸ºå•ä¸ªå­—ç¬¦ä¸²ä»¥ä¾¿å¤åˆ¶
    """
    lines = []

    # æ ‡é¢˜
    if results['title']:
        lines.append(f"å•†å“æ ‡é¢˜: {results['title']}")
    else:
        lines.append("å•†å“æ ‡é¢˜: æœªæ‰¾åˆ°")

    lines.append("-" * 40)

    # ä»·æ ¼
    if results['price']:
        lines.append(f"å•†å“ä»·æ ¼: {results['price']}")
    else:
        lines.append("å•†å“ä»·æ ¼: æœªæ‰¾åˆ°")

    lines.append("-" * 40)

    # è¯„åˆ†
    if results['rating']:
        rating = results['rating']
        lines.append("è¯„åˆ†ä¿¡æ¯:")
        if 'score' in rating:
            lines.append(f"  - è¯„åˆ†: {rating['score']}/5")
        if 'full_text' in rating:
            lines.append(f"  - è¯¦æƒ…: {rating['full_text']}")
        if 'review_count' in rating:
            lines.append(f"  - è¯„ä»·æ•°é‡: {rating['review_count']}")
    else:
        lines.append("è¯„åˆ†ä¿¡æ¯: æœªæ‰¾åˆ°")

    lines.append("-" * 40)

    # ç‰¹æ€§
    if results['features']:
        lines.append("å•†å“ç‰¹æ€§:")
        for i, feature in enumerate(results['features'], 1):
            lines.append(f"  {i}. {feature}")
    else:
        lines.append("å•†å“ç‰¹æ€§: æœªæ‰¾åˆ°")

    lines.append("-" * 40)

    # å•†å“è¯¦æƒ… (æ–°å¢)
    if results.get('product_details'):
        lines.append("å•†å“è¯¦æƒ…:")
        details = results['product_details']
        if details:
            for key, value in details.items():
                lines.append(f"  - {key}: {value}")
        else:
            lines.append("  æœªæ‰¾åˆ°å•†å“è¯¦æƒ…ã€‚")
    else:
        lines.append("å•†å“è¯¦æƒ…: æœªæ‰¾åˆ°")

    return "\n".join(lines)


# --- (æ–°å¢) æ ¼å¼åŒ–è¯„è®ºä¸ºç®€æ´æ–‡æœ¬ ---
def format_reviews_simple_text(reviews):
    """
    å°†è¯„è®ºåˆ—è¡¨æ ¼å¼åŒ–ä¸ºç®€æ´çš„æ–‡æœ¬å­—ç¬¦ä¸²
    """
    lines = []
    if not reviews:
        return "æœªæ‰¾åˆ°è¯„è®ºã€‚"

    for i, review in enumerate(reviews, 1):
        lines.append(f"ã€è¯„è®º {i}ã€‘")
        lines.append(f"ç”¨æˆ·å: {review.get('username', 'N/A')}")
        lines.append(f"è¯„åˆ†: {review.get('rating', '?')}/5")
        lines.append(f"æ—¥æœŸ: {review.get('date', 'N/A')}")
        lines.append(f"æ ‡é¢˜: {review.get('title', 'æ— æ ‡é¢˜')}")
        verified_text = "æ˜¯" if review.get('verified', False) else "å¦"
        lines.append(f"å·²éªŒè¯è´­ä¹°: {verified_text}")
        if review.get('helpful_count'):
            lines.append(f"æœ‰å¸®åŠ©: {review.get('helpful_count')}")
        lines.append(f"å†…å®¹: {review.get('content', 'æ— å†…å®¹')}")

        # (æ–°å¢) æ·»åŠ å›¾ç‰‡é“¾æ¥
        if review.get('images'):
            lines.append("å›¾ç‰‡é“¾æ¥:")
            for img_url in review.get('images'):
                lines.append(f"  - {img_url}")

        lines.append("\n" + "-" * 30 + "\n")

    return "\n".join(lines)


# --- ç”¨äºStreamlitçš„æ–°å‡½æ•° ---

def display_streamlit_results(results):
    """
    åœ¨Streamlité¡µé¢ä¸Šæ ¼å¼åŒ–æ˜¾ç¤ºæå–ç»“æœ (å·²ä¿®æ”¹)
    """
    st.header("æå–ç»“æœ")
    st.markdown("---")

    # --- 1. æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯ (æ–‡æœ¬æ¡†) ---
    st.subheader("ğŸ“¦ å•†å“åŸºæœ¬ä¿¡æ¯")
    other_info_text = format_other_info_for_display(results)
    # ä½¿ç”¨ disabled=True ä½¿å…¶åªè¯»ï¼Œä½†ç”¨æˆ·ä»å¯å¤åˆ¶
    st.text_area("åŸºæœ¬ä¿¡æ¯ (å¯å¤åˆ¶):", value=other_info_text, height=350, disabled=True, key="other_info_area")  # å¢åŠ äº†é«˜åº¦

    st.markdown("\n\n---\n")

    # --- 2. æ˜¾ç¤ºè¯„è®º (è¡¨æ ¼) ---
    st.subheader("ğŸ’¬ ç”¨æˆ·è¯„è®º")
    if results.get('reviews'):
        reviews_list = results['reviews']
        st.write(f"å…±æ‰¾åˆ° {len(reviews_list)} æ¡è¯„è®º")

        try:
            # --- 2a. åˆ›å»º DataFrame ---
            df = pd.DataFrame(reviews_list)

            # è°ƒæ•´åˆ—é¡ºåºä»¥ä¾¿æŸ¥çœ‹ (æ–°å¢ 'images')
            cols_order = ['username', 'rating', 'date', 'title', 'content', 'verified', 'helpful_count', 'images']
            # è¿‡æ»¤æ‰æ•°æ®ä¸­ä¸å­˜åœ¨çš„åˆ— (ä»¥é˜²ä¸‡ä¸€)
            existing_cols = [col for col in cols_order if col in df.columns]
            df_display = df[existing_cols]

            st.dataframe(df_display, use_container_width=True)

            # --- 2b. ä¸‹è½½æŒ‰é’® (CSV) ---
            csv_data = convert_df_to_csv(df_display)
            st.download_button(
                label="ä¸‹è½½è¯„è®º (CSV æ–‡ä»¶)",
                data=csv_data,
                file_name="amazon_reviews.csv",
                mime="text/csv",
                key="download_csv"
            )

            # --- 2c. (æ–°å¢) å›¾ç‰‡ä¸‹è½½ ---
            # æå–æ‰€æœ‰å”¯ä¸€çš„å›¾ç‰‡URL
            all_image_urls = sorted(list(set(
                url for review in reviews_list if review.get('images') for url in review['images']
            )))

            if all_image_urls:
                st.markdown("---")
                st.subheader(f"ğŸ“· è¯„è®ºå›¾ç‰‡ä¸‹è½½ (å…± {len(all_image_urls)} å¼ )")

                # ä½¿ç”¨ session state æ¥å­˜å‚¨ zip æ–‡ä»¶çš„çŠ¶æ€
                if 'zip_buffer' not in st.session_state:
                    st.session_state.zip_buffer = None

                if st.button("å‡†å¤‡å›¾ç‰‡ä¸‹è½½ (ZIP)", key="prep_download"):
                    # è°ƒç”¨å‡½æ•°å¹¶å­˜å‚¨ç»“æœ
                    # (ä½¿ç”¨ tuple æ˜¯å› ä¸º list ä¸å¯å“ˆå¸Œï¼Œæ— æ³•è¢« @st.cache_data ç¼“å­˜)
                    st.session_state.zip_buffer = download_and_zip_images(tuple(all_image_urls))

                # å¦‚æœ zip æ–‡ä»¶å·²ç»ç”Ÿæˆï¼Œæ˜¾ç¤ºæœ€ç»ˆçš„ä¸‹è½½æŒ‰é’®
                if st.session_state.zip_buffer:
                    st.download_button(
                        label="âœ… ç‚¹å‡»ä¸‹è½½ ZIP æ–‡ä»¶",
                        data=st.session_state.zip_buffer,
                        file_name="amazon_review_images.zip",
                        mime="application/zip",
                        key="download_zip_final"
                    )

            # --- 2d. ç®€æ´æ–‡æœ¬æ ¼å¼ ---
            st.markdown("---")
            st.subheader("ç®€æ´æ–‡æœ¬ç‰ˆè¯„è®º (å¯å¤åˆ¶)")
            simple_reviews_text = format_reviews_simple_text(reviews_list)
            st.text_area("è¯„è®ºæ–‡æœ¬:", value=simple_reviews_text, height=400, disabled=True, key="reviews_text_area")

        except Exception as e:
            st.error(f"å¤„ç†è¯„è®ºè¡¨æ ¼æ—¶å‡ºé”™: {e}")
            st.write("æ— æ³•ç”Ÿæˆè¡¨æ ¼ï¼Œæ˜¾ç¤ºåŸå§‹è¯„è®ºæ•°æ®ï¼š")
            st.json(reviews_list)  # ä½œä¸ºåå¤‡ï¼Œç›´æ¥æ‰“å°JSON

    else:
        st.info("æœªæ‰¾åˆ°ç”¨æˆ·è¯„è®ºã€‚")


# (æ­¤å‡½æ•°å·²è¢«ç§»é™¤ï¼ŒåŠŸèƒ½è¢« format_other_info_for_display å’Œ format_reviews_simple_text æ›¿ä»£)
# def format_results_for_copy(results):
#     ...

# --- Streamlit åº”ç”¨ç¨‹åºç•Œé¢ ---

# è®¾ç½®é¡µé¢æ ‡é¢˜
st.set_page_config(page_title="äºšé©¬é€Šä¿¡æ¯æå–å™¨", page_icon="ğŸ“¦")

st.title("ğŸ“¦ äºšé©¬é€Šå•†å“ä¿¡æ¯æå–å·¥å…·")

st.info("è¯·åœ¨å•†å“é¡µé¢å³é”® â†’ 'æŸ¥çœ‹ç½‘é¡µæºä»£ç ' â†’ å…¨é€‰ (Ctrl+A) â†’ å¤åˆ¶ (Ctrl+C) â†’ ç²˜è´´åˆ°ä¸‹æ–¹æ–‡æœ¬æ¡†ä¸­ã€‚")

# ç”¨äºç²˜è´´HTMLçš„æ–‡æœ¬åŒºåŸŸ
html_content = st.text_area("åœ¨æ­¤å¤„ç²˜è´´ç½‘é¡µæºä»£ç :", height=300, key="html_input", placeholder="å°†HTMLæºä»£ç ç²˜è´´åˆ°è¿™é‡Œ...")

# æå–æŒ‰é’®
if st.button("æå–ä¿¡æ¯", key="extract_button", type="primary"):
    if html_content:
        # (æ–°å¢) æ¸…ç†ä¸Šä¸€æ¬¡çš„ä¸‹è½½çŠ¶æ€
        if 'zip_buffer' in st.session_state:
            st.session_state.zip_buffer = None

        with st.spinner("æ­£åœ¨è§£æä¸­ï¼Œè¯·ç¨å€™..."):
            try:
                # æå–æ‰€æœ‰ä¿¡æ¯
                results = extract_all_product_info(html_content)

                # åœ¨é¡µé¢ä¸Šæ˜¾ç¤ºç»“æœ (ä½¿ç”¨ä¿®æ”¹åçš„å‡½æ•°)
                display_streamlit_results(results)

                # (æ—§çš„å¤åˆ¶éƒ¨åˆ†å·²ç§»é™¤)

                st.success("æå–å®Œæˆï¼")

            except Exception as e:
                st.error(f"è§£ææ—¶å‘ç”Ÿé”™è¯¯: {e}")
                st.exception(e)
    else:
        st.warning("è­¦å‘Šï¼šæ–‡æœ¬æ¡†ä¸ºç©ºï¼Œè¯·è¾“å…¥ç½‘é¡µæºä»£ç ã€‚")

