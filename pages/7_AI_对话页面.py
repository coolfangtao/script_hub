import streamlit as st
import google.generativeai as genai
from shared.sidebar import create_common_sidebar # å¯¼å…¥å…¬å…±ä¾§è¾¹æ å‡½æ•°

# --- é¡µé¢é…ç½®å’Œä¾§è¾¹æ  ---
st.set_page_config(
    page_title="AI å¯¹è¯",
    page_icon="ğŸ¤–"
)
create_common_sidebar() # è°ƒç”¨å‡½æ•°åˆ›å»ºä¾§è¾¹æ 

# --- API å¯†é’¥é…ç½® ---
# ä» Streamlit secrets ä¸­è·å– API å¯†é’¥
try:
    api_key = st.secrets["API_KEY"]
    genai.configure(api_key=api_key)
except (KeyError, FileNotFoundError):
    st.error("Gemini API å¯†é’¥æœªæ‰¾åˆ°ã€‚è¯·åœ¨ Streamlit secrets ä¸­è®¾ç½®åä¸º 'API_KEY' çš„å¯†é’¥ã€‚")
    st.stop()

# --- é¡µé¢æ ‡é¢˜å’Œä»‹ç» ---
st.title("ğŸ¤– AI å¯¹è¯")
st.caption("ä¸€ä¸ªç”± Google Gemini Pro é©±åŠ¨çš„èŠå¤©æœºå™¨äºº")


# --- æ¨¡å‹é€‰æ‹© ---
# å®šä¹‰å¯é€‰çš„ AI æ¨¡å‹åˆ—è¡¨
MODEL_OPTIONS = [
    "gemini-2.0-flash",
    "gemini-2.5-pro",
    "gemini-2.0-flash-exp",
    "gemini-2.0-flash-lite",
    "gemini-2.0-flash-preview-image-generation",
    "gemini-2.5-flash-lite",
    "gemini-2.5-flash-tts",
    "gemini-2.5-flash",
    "gemini-robotics-er-1.5-preview",
    "gemma-3-12b",
    "gemma-3-1b",
    "gemma-3-27b",
    "gemma-3-2b",
    "gemma-3-4b",
    "learnim-2.0-flash-experimental",
    "imagen-3.0-generate",
    "veo-2.0-generate-001",
    "gemini-2.0-flash-live",
    "gemini-2.5-flash-live",
    "gemini-2.5-flash-native-audio-dialog"
]

# åˆ›å»ºä¸€ä¸ªé€‰æ‹©æ¡†ï¼Œé»˜è®¤å€¼ä¸º 'gemini-2.0-flash'
# `index=0` å› ä¸º 'gemini-2.0-flash' æ˜¯åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
selected_model = st.selectbox(
    "è¯·é€‰æ‹©ä¸€ä¸ª AI æ¨¡å‹ï¼ˆhttps://aistudio.google.com/ï¼‰:",
    options=MODEL_OPTIONS,
    index=0
)


# --- åˆå§‹åŒ–æ¨¡å‹å’Œä¼šè¯çŠ¶æ€ ---
# æ£€æŸ¥ session_state ä¸­æ˜¯å¦å·²æœ‰èŠå¤©è®°å½•ï¼Œæˆ–è€…ç”¨æˆ·æ˜¯å¦åˆ‡æ¢äº†æ¨¡å‹
# å¦‚æœåˆ‡æ¢äº†æ¨¡å‹ï¼Œåˆ™éœ€è¦é‡ç½®èŠå¤©
if "gemini_chat" not in st.session_state or st.session_state.get("selected_model") != selected_model:
    # å­˜å‚¨å½“å‰é€‰æ‹©çš„æ¨¡å‹
    st.session_state.selected_model = selected_model
    # åˆå§‹åŒ–æ¨¡å‹
    model = genai.GenerativeModel(selected_model)
    # å¼€å§‹æ–°çš„èŠå¤©ï¼Œå¹¶å°†ä¼šè¯å¯¹è±¡å­˜å‚¨åœ¨ session_state ä¸­
    st.session_state.gemini_chat = model.start_chat(history=[])
    # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥å•ç‹¬å­˜å‚¨å’Œæ˜¾ç¤ºæ¶ˆæ¯
    st.session_state.messages = []
    # ï¼ˆå¯é€‰ï¼‰å¯ä»¥åŠ ä¸€ä¸ªæç¤ºï¼Œå‘Šè¯‰ç”¨æˆ·æ¨¡å‹å·²åˆ‡æ¢ï¼Œå¯¹è¯å·²é‡ç½®
    # st.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º {selected_model}ã€‚æ–°çš„å¯¹è¯å·²å¼€å§‹ã€‚")


# --- æ˜¾ç¤ºå†å²èŠå¤©è®°å½• ---
# éå† session_state ä¸­å­˜å‚¨çš„æ‰€æœ‰æ¶ˆæ¯å¹¶æ˜¾ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- èŠå¤©è¾“å…¥æ¡† ---
# Streamlit çš„ st.chat_input ç»„ä»¶ç”¨äºæ¥æ”¶ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("æ‚¨å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"):
    # 1. å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°æ¶ˆæ¯è®°å½•ä¸­å¹¶æ˜¾ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. è°ƒç”¨ Gemini API è·å–å›å¤
    try:
        # æ˜¾ç¤ºä¸€ä¸ªåŠ è½½æŒ‡ç¤ºå™¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
        with st.spinner("AI æ­£åœ¨æ€è€ƒä¸­..."):
            # ä½¿ç”¨å­˜å‚¨åœ¨ session_state ä¸­çš„èŠå¤©ä¼šè¯å¯¹è±¡å‘é€æ¶ˆæ¯
            response = st.session_state.gemini_chat.send_message(prompt, stream=True)
            response.resolve() # ç­‰å¾…æ‰€æœ‰æµå¼æ•°æ®å—æ¥æ”¶å®Œæ¯•

        # 3. å°† AI çš„å›å¤æ·»åŠ åˆ°æ¶ˆæ¯è®°å½•ä¸­å¹¶æ˜¾ç¤º
        ai_response = response.text
        st.session_state.messages.append({"role": "assistant", "content": ai_response})
        with st.chat_message("assistant"):
            st.markdown(ai_response)

    except Exception as e:
        st.error(f"è°ƒç”¨ API æ—¶å‡ºé”™: {e}")