import streamlit as st
import pandas as pd
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re
from collections import Counter
from typing import Optional, Dict, Tuple, List
from io import BytesIO
from shared.sidebar import create_common_sidebar # <-- 1. å¯¼å…¥å‡½æ•°
create_common_sidebar() # <-- 2. è°ƒç”¨å‡½æ•°ï¼Œç¡®ä¿æ¯ä¸ªé¡µé¢éƒ½æœ‰ä¾§è¾¹æ 


# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---
def parse_filename(filename: str) -> Optional[Dict[str, str]]:
    """
    ä»æ ‡å‡†æ–‡ä»¶åä¸­è§£æå‡ºASINç­‰ä¿¡æ¯ã€‚
    æ–‡ä»¶åæ ¼å¼: ReverseASIN-US-B01N9KSITZ(1584)-20251012.xlsx
    """
    pattern = re.compile(r'ReverseASIN-(.*?)-([A-Z0-9]{10})\(.*\)-\d+')
    match = pattern.match(filename)
    if match:
        return {'country': match.group(1), 'asin': match.group(2)}
    return None


def process_uploaded_file(uploaded_file) -> Optional[Tuple[str, pd.DataFrame, pd.DataFrame]]:
    """
    å¤„ç†å•ä¸ªä¸Šä¼ çš„Excelæ–‡ä»¶ã€‚
    1. è¯»å–ç¬¬ä¸€ä¸ªsheetã€‚
    2. è§£ææ–‡ä»¶åè·å–ASINã€‚
    3. åˆ›å»ºä¸€ä¸ªæ–°DataFrameï¼Œå¹¶åœ¨ç¬¬ä¸€åˆ—æ·»åŠ ASINä¿¡æ¯ã€‚

    Args:
        uploaded_file: Streamlitä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡ã€‚

    Returns:
        Optional[Tuple[str, pd.DataFrame, pd.DataFrame]]:
        ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«:
        - åŸå§‹sheetçš„åç§°ã€‚
        - åŸå§‹sheetçš„å®Œæ•´DataFrame (ç”¨äºå•ç‹¬å­˜æ”¾)ã€‚
        - æ·»åŠ äº†ASINåˆ—çš„DataFrame (ç”¨äºæ€»è¡¨åˆå¹¶)ã€‚
        å¦‚æœæ–‡ä»¶å¤„ç†å¤±è´¥æˆ–æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ™è¿”å›Noneã€‚
    """
    try:
        # è§£ææ–‡ä»¶å
        file_info = parse_filename(uploaded_file.name)
        if not file_info:
            st.warning(f"æ–‡ä»¶å '{uploaded_file.name}' æ ¼å¼ä¸ç¬¦åˆè¦æ±‚ï¼Œå·²è·³è¿‡ã€‚")
            return None

        asin = file_info['asin']

        # è¯»å–Excelçš„ç¬¬ä¸€ä¸ªsheet
        # `sheet_name=0` è¡¨ç¤ºè¯»å–ç¬¬ä¸€ä¸ªsheet
        # `engine='openpyxl'` æ˜¯ä¸ºäº†æ›´å¥½åœ°å…¼å®¹.xlsxæ–‡ä»¶
        original_df = pd.read_excel(uploaded_file, sheet_name=0, engine='openpyxl')

        # è·å–ç¬¬ä¸€ä¸ªsheetçš„åç§°ï¼Œç”¨äºåœ¨æ–°Excelä¸­åˆ›å»ºåŒåsheet
        xls = pd.ExcelFile(uploaded_file, engine='openpyxl')
        sheet_name = xls.sheet_names[0]

        # å‡†å¤‡ç”¨äºåˆå¹¶åˆ°æ€»è¡¨çš„æ•°æ®
        df_for_consolidation = original_df.copy()
        # åœ¨ç¬¬ä¸€åˆ—æ’å…¥ASINä¿¡æ¯
        df_for_consolidation.insert(0, 'ASIN', asin)

        return sheet_name, original_df, df_for_consolidation

    except Exception as e:
        st.error(f"å¤„ç†æ–‡ä»¶ '{uploaded_file.name}' æ—¶å‡ºé”™: {e}")
        return None


def create_excel_file(individual_sheets: Dict[str, pd.DataFrame], consolidated_df: pd.DataFrame) -> BytesIO:
    """
    å°†å¤„ç†å¥½çš„æ•°æ®å†™å…¥ä¸€ä¸ªæ–°çš„Excelæ–‡ä»¶ï¼ˆåœ¨å†…å­˜ä¸­ï¼‰ã€‚

    Args:
        individual_sheets (Dict[str, pd.DataFrame]): ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯sheetåï¼Œå€¼æ˜¯å¯¹åº”çš„åŸå§‹DataFrameã€‚
        consolidated_df (pd.DataFrame): æ•´åˆäº†æ‰€æœ‰ASINä¿¡æ¯çš„æ€»è¡¨DataFrameã€‚

    Returns:
        BytesIO: åŒ…å«æ–°Excelæ–‡ä»¶å†…å®¹çš„äºŒè¿›åˆ¶æµå¯¹è±¡ã€‚
    """
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        # 1. å†™å…¥æ€»è¡¨ï¼Œå¹¶å°†å…¶æ”¾åœ¨ç¬¬ä¸€ä¸ª
        consolidated_df.to_excel(writer, sheet_name='æ€»è¡¨-æ‰€æœ‰ASINæ•´åˆ', index=False)

        # 2. å†™å…¥æ¯ä¸ªç‹¬ç«‹çš„ASIN sheet
        for sheet_name, df in individual_sheets.items():
            df.to_excel(writer, sheet_name=sheet_name, index=False)

    # é‡ç½®æŒ‡é’ˆåˆ°æ–‡ä»¶å¼€å¤´ï¼Œä»¥ä¾¿st.download_buttonå¯ä»¥è¯»å–å®ƒ
    output.seek(0)
    return output


# --- æ•°æ®åˆ†æä¸å¯è§†åŒ–å‡½æ•° ---
def display_key_metrics(df: pd.DataFrame, is_consolidated: bool = False, asin: str = ""):
    """å±•ç¤ºå…³é”®æŒ‡æ ‡æ€»è§ˆã€‚"""
    st.subheader("å…³é”®æŒ‡æ ‡æ€»è§ˆ (Key Metrics)")

    total_keywords = df['æµé‡è¯'].nunique()
    total_search_volume = df['æœˆæœç´¢é‡'].sum()
    total_purchases = df['è´­ä¹°é‡'].sum()
    avg_purchase_rate = df['è´­ä¹°ç‡'].mean()

    cols = st.columns(4)
    with cols[0]:
        st.metric("å…³é”®è¯æ€»æ•°", f"{total_keywords:,}")
    with cols[1]:
        st.metric("æœˆæœç´¢æ€»é‡", f"{int(total_search_volume):,}")
    with cols[2]:
        st.metric("æœˆè´­ä¹°æ€»é‡", f"{int(total_purchases):,}")
    with cols[3]:
        st.metric("å¹³å‡è´­ä¹°ç‡", f"{avg_purchase_rate:.2%}")

    if is_consolidated:
        total_asins = df['ASIN'].nunique()
        st.info(f"å½“å‰æ•°æ®ä¸º **{total_asins}** ä¸ªASINçš„åˆå¹¶åˆ†æç»“æœã€‚")
    else:
        st.info(f"å½“å‰æ•°æ®ä¸ºå•ä¸ªASIN **{asin}** çš„åˆ†æç»“æœã€‚")


def plot_keyword_traffic(df: pd.DataFrame):
    """
    å±•ç¤ºå…³é”®è¯æµé‡çš„å †å æ¡å½¢å›¾ã€‚
    æ­¤å‡½æ•°ä¼šå…ˆèšåˆæ•°æ®ï¼Œä»¥æ­£ç¡®å¤„ç†å•æ–‡ä»¶å’Œåˆå¹¶åæ–‡ä»¶çš„æƒ…å†µã€‚
    """
    st.subheader("å…³é”®è¯æµé‡ (Keyword Traffic)")

    # è¿‡æ»¤æ‰æµé‡å æ¯”ä¸º0çš„æ•°æ®
    df_filtered = df[df['æµé‡å æ¯”'] > 0].copy()
    if df_filtered.empty:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æµé‡æ•°æ®å¯ä¾›å±•ç¤ºã€‚")
        return

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ASINåˆ—
    has_asin = 'ASIN' in df_filtered.columns

    # --- ä¿®æ”¹åçš„èšåˆé€»è¾‘ ---
    # 1. é¦–å…ˆè®¡ç®—æ¯è¡Œçš„ç»å¯¹æµé‡è´¡çŒ®
    df_filtered["è‡ªç„¶æµé‡ç»å¯¹å æ¯”"] = df_filtered["æµé‡å æ¯”"] * df_filtered["è‡ªç„¶æµé‡å æ¯”"]
    df_filtered["å¹¿å‘Šæµé‡ç»å¯¹å æ¯”"] = df_filtered["æµé‡å æ¯”"] * df_filtered["å¹¿å‘Šæµé‡å æ¯”"]

    # 2. æŒ‰"æµé‡è¯"åˆ†ç»„å¹¶åˆè®¡ç›¸å…³æŒ‡æ ‡ï¼Œå¦‚æœæœ‰å¤šASINæ•°æ®åˆ™æ˜¾ç¤ºASINæ•°é‡
    if has_asin:
        # å¤šASINæƒ…å†µï¼šæŒ‰æµé‡è¯èšåˆï¼Œå¹¶è®¡ç®—æ¶‰åŠçš„ASINæ•°é‡
        aggregation_config = {
            "æµé‡å æ¯”": "sum",
            "è‡ªç„¶æµé‡ç»å¯¹å æ¯”": "sum",
            "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”": "sum",
            "ASIN": "nunique"  # è®¡ç®—ä¸åŒASINçš„æ•°é‡
        }
        groupby_columns = ["æµé‡è¯"]
    else:
        # å•ASINæƒ…å†µï¼šåªèšåˆæµé‡æ•°æ®
        aggregation_config = {
            "æµé‡å æ¯”": "sum",
            "è‡ªç„¶æµé‡ç»å¯¹å æ¯”": "sum",
            "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”": "sum"
        }
        groupby_columns = ["æµé‡è¯"]

    aggregated_df = df_filtered.groupby(groupby_columns).agg(aggregation_config).reset_index()

    # é‡å‘½ååˆ—ä»¥ä¾¿æ¸…æ™°æ˜¾ç¤º
    if has_asin:
        aggregated_df = aggregated_df.rename(columns={"ASIN": "æ¶‰åŠASINæ•°é‡"})
    # --- èšåˆé€»è¾‘ç»“æŸ ---

    # ä»èšåˆåçš„æ•°æ®ä¸­é€‰å–Top 20
    top_20_traffic = aggregated_df.sort_values(by="æµé‡å æ¯”", ascending=False).head(20)

    # è®¾ç½®å›¾è¡¨æ ‡é¢˜
    title_suffix = " (å¤šASINæ±‡æ€»)" if has_asin else ""
    chart_title = f"Top 20 å…³é”®è¯æµé‡åˆ†å¸ƒ{title_suffix}"

    # èåŒ–èšåˆåçš„DataFrameä»¥é€‚é…Plotlyçš„æ ¼å¼
    plot_df = top_20_traffic.melt(
        id_vars=["æµé‡è¯"],
        value_vars=["è‡ªç„¶æµé‡ç»å¯¹å æ¯”", "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”"],
        var_name="æµé‡ç±»å‹",
        value_name="å æ¯”"
    )

    # å‡†å¤‡æ‚¬åœæ•°æ®
    hover_data = {}
    if has_asin:
        # ä¸ºæ¯ä¸ªå…³é”®è¯æ·»åŠ ASINæ•°é‡ä¿¡æ¯åˆ°æ‚¬åœæ•°æ®
        asin_count_map = top_20_traffic.set_index('æµé‡è¯')['æ¶‰åŠASINæ•°é‡'].to_dict()
        plot_df['æ¶‰åŠASINæ•°é‡'] = plot_df['æµé‡è¯'].map(asin_count_map)
        hover_data = {'æ¶‰åŠASINæ•°é‡': True}

    fig = px.bar(
        plot_df, y="æµé‡è¯", x="å æ¯”", color="æµé‡ç±»å‹", orientation='h',
        title=chart_title,
        labels={"æµé‡è¯": "å…³é”®è¯", "å æ¯”": "æµé‡å æ¯”"},
        color_discrete_map={"è‡ªç„¶æµé‡ç»å¯¹å æ¯”": "#636EFA", "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”": "#EF553B"},
        text="å æ¯”",
        hover_data=hover_data
    )

    # æ›´æ–°å †å æŸ±å­çš„æ–‡æœ¬æ˜¾ç¤º
    fig.update_traces(texttemplate='%{text:.2%}', textposition='inside', insidetextanchor='middle')

    # --- æ–°å¢ï¼šåœ¨é¡¶éƒ¨æ·»åŠ æ€»æµé‡å æ¯”æ ‡æ³¨ ---
    # ä¸ºæ¯ä¸ªå…³é”®è¯æ·»åŠ æ€»æµé‡å æ¯”çš„æ ‡æ³¨
    for i, row in top_20_traffic.iterrows():
        annotation_text = f"{row['æµé‡å æ¯”']:.2%}"
        if has_asin:
            annotation_text += f" ({row['æ¶‰åŠASINæ•°é‡']}ä¸ªASIN)"

        fig.add_annotation(
            x=row['æµé‡å æ¯”'],  # xä½ç½®ä¸ºæ€»æµé‡å æ¯”
            y=row['æµé‡è¯'],  # yä½ç½®ä¸ºå…³é”®è¯
            text=annotation_text,  # æ˜¾ç¤ºæ€»æµé‡å æ¯”å’ŒASINæ•°é‡
            showarrow=False,
            xanchor='left',
            xshift=10,  # å‘å³åç§»ä¸€ç‚¹ï¼Œé¿å…ä¸æŸ±å­é‡å 
            font=dict(color='green', size=12, weight='bold'),
        )

    fig.update_layout(
        xaxis_title="æµé‡å æ¯”", yaxis_title="å…³é”®è¯",
        yaxis={'categoryorder': 'total ascending'}, height=800,
        legend_title_text='æµé‡ç±»å‹',
        xaxis=dict(tickformat=".2%")
    )
    st.plotly_chart(fig, use_container_width=True)


def plot_search_volume_and_purchases(df: pd.DataFrame):
    """å±•ç¤ºå…³é”®è¯æœç´¢é‡å’Œè´­ä¹°é‡çš„å †å æ¡å½¢å›¾ã€‚"""
    st.subheader("å…³é”®è¯æœç´¢é‡å’Œè´­ä¹°é‡ (Search Volume and Purchases)")

    df_filtered = df[df['æœˆæœç´¢é‡'] > 0].copy()
    if df_filtered.empty:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æœˆæœç´¢é‡æ•°æ®å¯ä¾›å±•ç¤ºã€‚")
        return

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ASINåˆ—
    has_asin = 'ASIN' in df_filtered.columns

    # å¤šASINæƒ…å†µéœ€è¦å…ˆæŒ‰å…³é”®è¯èšåˆ
    if has_asin:
        # èšåˆå¤šASINæ•°æ®
        aggregated_df = df_filtered.groupby("æµé‡è¯").agg({
            "æœˆæœç´¢é‡": "sum",
            "è´­ä¹°é‡": "sum",
            "è´­ä¹°ç‡": "mean",  # è´­ä¹°ç‡å–å¹³å‡å€¼
            "ASIN": "nunique"  # è®¡ç®—ä¸åŒASINçš„æ•°é‡
        }).reset_index().rename(columns={"ASIN": "æ¶‰åŠASINæ•°é‡"})

        # é‡æ–°è®¡ç®—è´­ä¹°ç‡ï¼ˆåŸºäºèšåˆåçš„æ•°æ®ï¼‰
        aggregated_df["è´­ä¹°ç‡"] = aggregated_df["è´­ä¹°é‡"] / aggregated_df["æœˆæœç´¢é‡"]
        top_20_search = aggregated_df.sort_values(by="æœˆæœç´¢é‡", ascending=False).head(20)
    else:
        # å•ASINæƒ…å†µç›´æ¥ä½¿ç”¨åŸæ•°æ®
        top_20_search = df_filtered.sort_values(by="æœˆæœç´¢é‡", ascending=False).head(20).copy()

    top_20_search["æœªè´­ä¹°é‡"] = top_20_search["æœˆæœç´¢é‡"] - top_20_search["è´­ä¹°é‡"]

    # è®¾ç½®å›¾è¡¨æ ‡é¢˜
    title_suffix = " (å¤šASINæ±‡æ€»)" if has_asin else ""
    chart_title = f"Top 20 å…³é”®è¯æœç´¢é‡ä¸è´­ä¹°é‡{title_suffix}"

    plot_df = top_20_search.melt(
        id_vars=["æµé‡è¯", "è´­ä¹°ç‡"],
        value_vars=["è´­ä¹°é‡", "æœªè´­ä¹°é‡"],
        var_name="ç±»å‹", value_name="æ•°é‡"
    )

    # å‡†å¤‡æ‚¬åœæ•°æ®
    hover_data = {}
    if has_asin:
        asin_count_map = top_20_search.set_index('æµé‡è¯')['æ¶‰åŠASINæ•°é‡'].to_dict()
        plot_df['æ¶‰åŠASINæ•°é‡'] = plot_df['æµé‡è¯'].map(asin_count_map)
        hover_data = {'æ¶‰åŠASINæ•°é‡': True}

    fig = px.bar(
        plot_df, y="æµé‡è¯", x="æ•°é‡", color="ç±»å‹", orientation='h',
        title=chart_title,
        labels={"æµé‡è¯": "å…³é”®è¯", "æ•°é‡": "æœˆæœç´¢é‡"},
        color_discrete_map={"è´­ä¹°é‡": "#00CC96", "æœªè´­ä¹°é‡": "#FECB52"},
        hover_data=hover_data
    )

    # ä¸ºæ¯ä¸ªæ¡å½¢æ·»åŠ è´­ä¹°ç‡æ ‡æ³¨
    annotations = []
    for _, row in top_20_search.iterrows():
        # è´­ä¹°ç‡æ ‡æ³¨ï¼ˆåœ¨æŸ±å­å†…éƒ¨å³ä¾§ï¼‰
        purchase_rate_text = f"è´­ä¹°ç‡: {row['è´­ä¹°ç‡']:.2%}"
        if has_asin:
            purchase_rate_text += f" ({row['æ¶‰åŠASINæ•°é‡']}ä¸ªASIN)"

        annotations.append(dict(
            x=row['æœˆæœç´¢é‡'] * 0.98, y=row['æµé‡è¯'],
            text=purchase_rate_text,
            showarrow=False, font=dict(color="black", size=10),
            xanchor='right'
        ))

        # æ–°å¢ï¼šæ€»æœç´¢é‡æ ‡æ³¨ï¼ˆåœ¨æŸ±å­é¡¶ç«¯ï¼‰
        search_volume_text = f"{row['æœˆæœç´¢é‡']:,}"  # æ ¼å¼åŒ–æ•°å­—ï¼Œæ·»åŠ åƒä½åˆ†éš”ç¬¦
        annotations.append(dict(
            x=row['æœˆæœç´¢é‡'], y=row['æµé‡è¯'],
            text=search_volume_text,
            showarrow=False,
            xanchor='left',
            xshift=10,  # å‘å³åç§»ä¸€ç‚¹ï¼Œé¿å…ä¸æŸ±å­é‡å 
            font=dict(color='green', size=12, weight='bold'),
        ))

    fig.update_layout(annotations=annotations)

    fig.update_layout(
        xaxis_title="æœˆæœç´¢é‡", yaxis_title="å…³é”®è¯",
        yaxis={'categoryorder': 'total ascending'}, height=800,
        legend_title_text='ç±»å‹'
    )
    st.plotly_chart(fig, use_container_width=True)


def plot_keyword_analysis(df: pd.DataFrame):
    """ç»¼åˆåˆ†æå…³é”®è¯çš„æœç´¢é‡å’Œæµé‡å æ¯”ï¼Œè¯†åˆ«ä¸åŒç±»å‹çš„å…³é”®è¯"""
    st.subheader("å…³é”®è¯ç»¼åˆåˆ†æ (Keyword Analysis)")

    # æ•°æ®é¢„å¤„ç†
    df_filtered = df[(df['æœˆæœç´¢é‡'] > 0) & (df['æµé‡å æ¯”'] > 0)].copy()
    if df_filtered.empty:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æœç´¢é‡å’Œæµé‡æ•°æ®å¯ä¾›åˆ†æã€‚")
        return

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ASINåˆ—
    has_asin = 'ASIN' in df_filtered.columns

    # å¤šASINæƒ…å†µéœ€è¦å…ˆèšåˆæ•°æ®
    if has_asin:
        # èšåˆå¤šASINæ•°æ®
        aggregated_df = df_filtered.groupby("æµé‡è¯").agg({
            "æœˆæœç´¢é‡": "sum",
            "æµé‡å æ¯”": "sum",
            "è´­ä¹°ç‡": "mean",
            "è‡ªç„¶æµé‡å æ¯”": "mean",
            "å¹¿å‘Šæµé‡å æ¯”": "mean",
            "ASIN": "nunique"
        }).reset_index().rename(columns={"ASIN": "æ¶‰åŠASINæ•°é‡"})

        # é‡æ–°è®¡ç®—è´­ä¹°ç‡ï¼ˆåŸºäºèšåˆåçš„æ•°æ®ï¼‰
        df_filtered = aggregated_df
        st.info("ğŸ“Š å½“å‰æ˜¾ç¤ºå¤šASINæ±‡æ€»æ•°æ®ï¼Œå·²æŒ‰å…³é”®è¯èšåˆ")

    # è®¡ç®—æŒ‡æ ‡
    df_filtered['æ€»æµé‡è´¡çŒ®'] = df_filtered['æµé‡å æ¯”'] * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”ä¾¿äºåˆ†æ

    # è®¡ç®—æœç´¢é‡å’Œæµé‡å æ¯”çš„ä¸­ä½æ•°ä½œä¸ºåˆ†ç•Œçº¿
    search_median = df_filtered['æœˆæœç´¢é‡'].median()
    traffic_median = df_filtered['æ€»æµé‡è´¡çŒ®'].median()

    # åˆ†ç±»å…³é”®è¯
    def classify_keyword(row):
        search_vol = row['æœˆæœç´¢é‡']
        traffic_contribution = row['æ€»æµé‡è´¡çŒ®']

        if search_vol > search_median and traffic_contribution > traffic_median:
            return 'æ ¸å¿ƒå¤§è¯ (é«˜æœç´¢é‡+é«˜æµé‡)'
        elif search_vol > search_median and traffic_contribution <= traffic_median:
            return 'æ½œåŠ›è¯ (é«˜æœç´¢é‡+ä½æµé‡)'
        elif search_vol <= search_median and traffic_contribution > traffic_median:
            return 'ç²¾å‡†è¯ (ä½æœç´¢é‡+é«˜æµé‡)'
        else:
            return 'é•¿å°¾è¯ (ä½æœç´¢é‡+ä½æµé‡)'

    df_filtered['å…³é”®è¯ç±»å‹'] = df_filtered.apply(classify_keyword, axis=1)

    # è®¾ç½®å›¾è¡¨æ ‡é¢˜
    title_suffix = " (å¤šASINæ±‡æ€»)" if has_asin else ""
    chart_title = f'å…³é”®è¯æœç´¢é‡ vs æµé‡å æ¯”åˆ†æ{title_suffix}'

    # åˆ›å»ºè‡ªå®šä¹‰æ‚¬åœæ¨¡æ¿
    hover_template = (
        "<span style='font-size: 16px; font-weight: bold; color: green'>ğŸ“Œ %{customdata[0]}</span><br><br>"
        "<b>æœç´¢é‡:</b> %{x:,}<br>"
        "<b>æµé‡å æ¯”:</b> %{y:.2f}%<br>"
        "<b>è´­ä¹°ç‡:</b> %{customdata[1]:.2%}<br>"
        "<b>è‡ªç„¶æµé‡:</b> %{customdata[2]:.2%}<br>"
        "<b>å¹¿å‘Šæµé‡:</b> %{customdata[3]:.2%}"
    )

    # å¦‚æœæœ‰å¤šASINæ•°æ®ï¼Œåœ¨æ‚¬åœæ¨¡æ¿ä¸­æ·»åŠ ASINæ•°é‡
    if has_asin:
        hover_template += "<br><b>æ¶‰åŠASINæ•°é‡:</b> %{customdata[4]}"

    hover_template += "<extra></extra>"

    # å‡†å¤‡æ‚¬åœæ•°æ®
    hover_data = ['æµé‡è¯', 'è´­ä¹°ç‡', 'è‡ªç„¶æµé‡å æ¯”', 'å¹¿å‘Šæµé‡å æ¯”']
    if has_asin:
        hover_data.append('æ¶‰åŠASINæ•°é‡')

    # åˆ›å»ºæ•£ç‚¹å›¾ - ä¿®å¤ç‰ˆæœ¬
    fig = px.scatter(
        df_filtered,
        x='æœˆæœç´¢é‡',
        y='æ€»æµé‡è´¡çŒ®',
        color='å…³é”®è¯ç±»å‹',
        title=chart_title,
        labels={
            'æœˆæœç´¢é‡': 'æœˆæœç´¢é‡',
            'æ€»æµé‡è´¡çŒ®': 'æµé‡å æ¯” (%)',
            'å…³é”®è¯ç±»å‹': 'å…³é”®è¯ç±»å‹'
        },
        color_discrete_map={
            'æ ¸å¿ƒå¤§è¯ (é«˜æœç´¢é‡+é«˜æµé‡)': '#FF6B6B',
            'æ½œåŠ›è¯ (é«˜æœç´¢é‡+ä½æµé‡)': '#FFD93D',
            'ç²¾å‡†è¯ (ä½æœç´¢é‡+é«˜æµé‡)': '#6BCF77',
            'é•¿å°¾è¯ (ä½æœç´¢é‡+ä½æµé‡)': '#4D96FF'
        }
    )

    # è®¾ç½®æ‰€æœ‰ç‚¹çš„å¤§å°ç›¸åŒ
    fig.update_traces(marker=dict(size=12))

    # æ›´æ–°æ‚¬åœæ¨¡æ¿
    fig.update_traces(
        hovertemplate=hover_template,
        customdata=df_filtered[hover_data]
    )

    # æ·»åŠ ä¸­ä½çº¿
    fig.add_hline(y=traffic_median, line_dash="dash", line_color="red",
                  annotation_text=f"æµé‡ä¸­ä½æ•°: {traffic_median:.2f}%")
    fig.add_vline(x=search_median, line_dash="dash", line_color="red",
                  annotation_text=f"æœç´¢é‡ä¸­ä½æ•°: {search_median:,.0f}")

    fig.update_layout(height=600)
    st.plotly_chart(fig, use_container_width=True)

    # æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡å’Œè¯¦ç»†æ•°æ®
    col1, col2 = st.columns(2)

    with col1:
        st.write("### ğŸ“Š åˆ†ç±»ç»Ÿè®¡")
        type_stats = df_filtered['å…³é”®è¯ç±»å‹'].value_counts()
        for type_name, count in type_stats.items():
            percentage = count / len(df_filtered) * 100
            st.write(f"**{type_name}**: {count}ä¸ª ({percentage:.1f}%)")

    with col2:
        st.write("### ğŸ“ˆ å…³é”®æŒ‡æ ‡")
        st.metric("æ€»å…³é”®è¯æ•°", len(df_filtered))
        st.metric("æœç´¢é‡ä¸­ä½æ•°", f"{search_median:,.0f}")
        st.metric("æµé‡å æ¯”ä¸­ä½æ•°", f"{traffic_median:.2f}%")
        if has_asin:
            total_asin = df_filtered['æ¶‰åŠASINæ•°é‡'].sum() if 'æ¶‰åŠASINæ•°é‡' in df_filtered.columns else 'N/A'
            st.metric("æ€»æ¶‰åŠASINæ•°", total_asin)

    # æ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨æ ¼
    st.write("### ğŸ“‹ è¯¦ç»†æ•°æ®")

    # æ·»åŠ åˆ†ç±»ç­›é€‰å™¨
    selected_types = st.multiselect(
        "é€‰æ‹©è¦æ˜¾ç¤ºçš„å…³é”®è¯ç±»å‹:",
        options=df_filtered['å…³é”®è¯ç±»å‹'].unique(),
        default=df_filtered['å…³é”®è¯ç±»å‹'].unique()
    )

    filtered_df = df_filtered[df_filtered['å…³é”®è¯ç±»å‹'].isin(selected_types)]

    # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
    display_columns = ['æµé‡è¯', 'æœˆæœç´¢é‡', 'æµé‡å æ¯”', 'è´­ä¹°ç‡', 'è‡ªç„¶æµé‡å æ¯”', 'å¹¿å‘Šæµé‡å æ¯”', 'å…³é”®è¯ç±»å‹']
    if has_asin and 'æ¶‰åŠASINæ•°é‡' in filtered_df.columns:
        display_columns.append('æ¶‰åŠASINæ•°é‡')

    display_df = filtered_df[display_columns].sort_values(['æœˆæœç´¢é‡', 'æµé‡å æ¯”'], ascending=[False, False])

    # æ ¼å¼åŒ–æ˜¾ç¤º
    formatted_df = display_df.copy()
    formatted_df['æœˆæœç´¢é‡'] = formatted_df['æœˆæœç´¢é‡'].apply(lambda x: f"{x:,.0f}")
    formatted_df['æµé‡å æ¯”'] = formatted_df['æµé‡å æ¯”'].apply(lambda x: f"{x:.2%}")
    formatted_df['è´­ä¹°ç‡'] = formatted_df['è´­ä¹°ç‡'].apply(lambda x: f"{x:.2%}")
    formatted_df['è‡ªç„¶æµé‡å æ¯”'] = formatted_df['è‡ªç„¶æµé‡å æ¯”'].apply(lambda x: f"{x:.2%}")
    formatted_df['å¹¿å‘Šæµé‡å æ¯”'] = formatted_df['å¹¿å‘Šæµé‡å æ¯”'].apply(lambda x: f"{x:.2%}")

    st.dataframe(formatted_df, use_container_width=True)

    # æä¾›ä¸‹è½½åŠŸèƒ½
    csv = filtered_df[display_columns].to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½ç­›é€‰åçš„æ•°æ® (CSV)",
        data=csv,
        file_name="keyword_analysis.csv",
        mime="text/csv"
    )

    # ç­–ç•¥å»ºè®®
    st.write("### ğŸ’¡ ç­–ç•¥å»ºè®®")

    advice_mapping = {
        'æ ¸å¿ƒå¤§è¯ (é«˜æœç´¢é‡+é«˜æµé‡)':
            "âœ… **ä¿æŒä¼˜åŠ¿**: è¿™äº›æ˜¯æ‚¨çš„æ ¸å¿ƒå…³é”®è¯ï¼Œç»§ç»­ä¿æŒä¼˜åŒ–ï¼Œè€ƒè™‘å¢åŠ ç›¸å…³é•¿å°¾è¯",

        'æ½œåŠ›è¯ (é«˜æœç´¢é‡+ä½æµé‡)':
            "ğŸš€ **é‡ç‚¹çªç ´**: é«˜æœç´¢é‡ä½†æµé‡ä½ï¼Œéœ€è¦ä¼˜åŒ–é¡µé¢å†…å®¹ã€æé«˜æ’åæˆ–å¢åŠ å¹¿å‘ŠæŠ•å…¥",

        'ç²¾å‡†è¯ (ä½æœç´¢é‡+é«˜æµé‡)':
            "ğŸ¯ **ç²¾å‡†ç»´æŠ¤**: è½¬åŒ–ç‡é«˜ï¼Œç»´æŠ¤ç°æœ‰æ’åï¼Œå¯é€‚å½“æ‹“å±•ç›¸å…³è¯",

        'é•¿å°¾è¯ (ä½æœç´¢é‡+ä½æµé‡)':
            "ğŸ“ˆ **é€‰æ‹©æ€§ä¼˜åŒ–**: ç«äº‰è¾ƒå°ï¼Œå¯ä½œä¸ºè¡¥å……ï¼Œé‡ç‚¹å…³æ³¨æœ‰è½¬åŒ–æ½œåŠ›çš„è¯"
    }

    for keyword_type in selected_types:
        if keyword_type in advice_mapping:
            st.info(f"**{keyword_type}**: {advice_mapping[keyword_type]}")



def calculate_word_frequency(df: pd.DataFrame) -> Tuple[Dict[str, int], pd.DataFrame]:
    """ä»æµé‡è¯åˆ—è®¡ç®—å•è¯é¢‘ç‡ã€‚"""
    words = df['æµé‡è¯'].dropna().str.lower().str.split().sum()
    word_counts = Counter(words)
    total_words = sum(word_counts.values())

    freq_df = pd.DataFrame(word_counts.items(), columns=['å•è¯', 'å‡ºç°æ¬¡æ•°'])
    freq_df['é¢‘ç‡'] = freq_df['å‡ºç°æ¬¡æ•°'] / total_words
    freq_df = freq_df.sort_values(by='å‡ºç°æ¬¡æ•°', ascending=False).reset_index(drop=True)

    return word_counts, freq_df


def display_word_cloud(word_counts):
    st.write("å•è¯è¯äº‘")
    wordcloud = WordCloud(
        width=1600,  # å¢å¤§ç”»å¸ƒå°ºå¯¸
        height=900,
        background_color=None,
        mode="RGBA",
        # max_words=200,  # é™åˆ¶å•è¯æ•°é‡
        max_font_size=200,  # è®¾ç½®æœ€å¤§å­—ä½“
        min_font_size=10,  # è®¾ç½®æœ€å°å­—ä½“
        prefer_horizontal=0.9,  # è°ƒæ•´æ°´å¹³æ˜¾ç¤ºåå¥½
        # relative_scaling=0.5,  # è°ƒæ•´ç¼©æ”¾æ¯”ä¾‹
        collocations=False  # ç¦ç”¨è¯ç»„
    ).generate_from_frequencies(word_counts)

    # åˆ›å»ºå›¾å½¢æ—¶è®¾ç½®æ›´é«˜çš„DPI
    fig, ax = plt.subplots(figsize=(12, 9), dpi=300)
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')

    # è®¾ç½®å›¾å½¢èƒŒæ™¯é€æ˜
    fig.patch.set_alpha(0)
    ax.patch.set_alpha(0)

    # ä¿å­˜ä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒ
    st.pyplot(fig, bbox_inches='tight', pad_inches=0, dpi=300)

def display_frequency_table(freq_df):
    """
    å±•ç¤ºæ ¼å¼åŒ–åçš„å•è¯é¢‘ç‡ç»Ÿè®¡è¡¨æ ¼ã€‚

    Args:
        freq_df (pd.DataFrame): åŒ…å«å•è¯ã€å‡ºç°æ¬¡æ•°å’Œé¢‘ç‡çš„DataFrameã€‚
    """
    st.write("å•è¯é¢‘ç‡ç»Ÿè®¡")
    st.dataframe(
        freq_df.head(20).style.format({"é¢‘ç‡": "{:.2%}"}),
        height=600,
        use_container_width=True
    )

def display_aggregated_word_frequency(df: pd.DataFrame):
    """å±•ç¤ºå•è¯é¢‘ç‡è¡¨æ ¼å’Œè¯äº‘ã€‚"""
    # --- è¯é¢‘åˆ†æ ---
    # 1. è®¡ç®—è¯é¢‘ (å…¬å…±é€»è¾‘)
    text = ' '.join(df['æµé‡è¯'].dropna())
    words = re.findall(r'\b\w+\b', text.lower())
    word_counts = Counter(words)
    total_words = sum(word_counts.values())
    freq_df = pd.DataFrame(word_counts.items(), columns=["å•è¯", "å‡ºç°æ¬¡æ•°"])
    freq_df["é¢‘ç‡"] = freq_df["å‡ºç°æ¬¡æ•°"] / total_words
    freq_df = freq_df.sort_values(by="å‡ºç°æ¬¡æ•°", ascending=False)

    # 2. ä¾æ¬¡å±•ç¤ºè¯äº‘å’Œè¡¨æ ¼
    display_word_cloud(word_counts)
    display_frequency_table(freq_df)


def display_raw_data(df: pd.DataFrame, title: str = "åŸå§‹æ•°æ® (Raw Data)"):
    """å±•ç¤ºåŸå§‹æ•°æ®è¡¨æ ¼ã€‚"""
    st.subheader(title)
    st.dataframe(df)


# --- åˆå¹¶æ–‡ä»¶åçš„ä¸“å±å¯è§†åŒ–å‡½æ•° ---

def plot_asin_traffic_contribution(df: pd.DataFrame):
    """å±•ç¤ºå„ASINæµé‡è´¡çŒ®å¯¹æ¯”çš„æŸ±çŠ¶å›¾ã€‚"""
    st.subheader("å„ASINæµé‡è´¡çŒ®å¯¹æ¯” (ASIN Traffic Contribution)")

    # è®¡ç®—æ¯ä¸ªASINçš„æ€»æµé‡è´¡çŒ®å€¼ï¼ˆæµé‡å æ¯” * æœˆæœç´¢é‡ï¼‰
    df['æµé‡è´¡çŒ®å€¼'] = df['æµé‡å æ¯”'] * df['æœˆæœç´¢é‡']
    asin_traffic = df.groupby('ASIN')['æµé‡è´¡çŒ®å€¼'].sum().sort_values(ascending=False).reset_index()

    fig = px.bar(
        asin_traffic, x='ASIN', y='æµé‡è´¡çŒ®å€¼',
        title="å„ASINæµé‡è´¡çŒ®å€¼å¯¹æ¯”",
        labels={'ASIN': 'äº§å“ASIN', 'æµé‡è´¡çŒ®å€¼': 'æµé‡è´¡çŒ®å€¼ (æµé‡å æ¯” * æœˆæœç´¢é‡)'},
        text='æµé‡è´¡çŒ®å€¼'
    )
    fig.update_traces(texttemplate='%{text:,.0f}', textposition='outside')
    fig.update_layout(xaxis_tickangle=-45)
    st.plotly_chart(fig, use_container_width=True)


# --- Streamlit é¡µé¢ä¸»å‡½æ•° ---

def main():
    st.set_page_config(layout="wide", page_title="ASINå…³é”®è¯åˆ†æé¢æ¿")
    st.title("ğŸ“Š ASINåæŸ¥å…³é”®è¯æ•°æ®åˆ†æé¢æ¿")

    uploaded_files = st.file_uploader(
        "è¯·ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªASINåæŸ¥çš„Excelæ–‡ä»¶",
        type="xlsx",
        accept_multiple_files=True
    )

    if not uploaded_files:
        st.info("è¯·ä¸Šä¼ æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")
        return

    # --- å•æ–‡ä»¶å¤„ç†é€»è¾‘ ---
    if len(uploaded_files) == 1:
        uploaded_file = uploaded_files[0]
        result = process_uploaded_file(uploaded_file)
        if result:
            _, df, _ = result
            asin_info = parse_filename(uploaded_file.name)
            asin = asin_info['asin'] if asin_info else "æœªçŸ¥"

            st.success(f"æˆåŠŸåŠ è½½æ–‡ä»¶: **{uploaded_file.name}**")

            # å•æ–‡ä»¶ä»ªè¡¨ç›˜
            display_key_metrics(df, asin=asin) # todoï¼šæ–°å¢ä¸“ç”¨å‡½æ•°

            # å±•ç¤ºæµé‡å æ¯”
            plot_keyword_traffic(df)

            # å±•ç¤ºæœç´¢å’Œè´­ä¹°å æ¯”
            plot_search_volume_and_purchases(df)

            plot_keyword_analysis(df)  # æ–°å¢çš„ç»¼åˆåˆ†æ

            # --- è¯é¢‘åˆ†æ ---
            st.subheader("å•ASINç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡ (Word Frequency)")
            display_aggregated_word_frequency(df)

            display_raw_data(df)

    # --- å¤šæ–‡ä»¶å¤„ç†é€»è¾‘ ---
    elif len(uploaded_files) > 1:
        individual_sheets = {}
        dfs_for_consolidation = []

        with st.spinner("æ­£åœ¨å¤„ç†å’Œåˆå¹¶æ–‡ä»¶..."):
            for file in uploaded_files:
                result = process_uploaded_file(file)
                if result:
                    sheet_name, original_df, df_for_consolidation = result
                    individual_sheets[sheet_name] = original_df
                    dfs_for_consolidation.append(df_for_consolidation)

        if not dfs_for_consolidation:
            st.error("æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶éƒ½å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
            return

        consolidated_df = pd.concat(dfs_for_consolidation, ignore_index=True)
        st.success(f"æˆåŠŸåˆå¹¶ **{len(dfs_for_consolidation)}** ä¸ªæ–‡ä»¶ï¼")

        # æä¾›åˆå¹¶åçš„æ–‡ä»¶ä¸‹è½½
        excel_bytes = create_excel_file(individual_sheets, consolidated_df)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½åˆå¹¶åçš„Excelæ–‡ä»¶",
            data=excel_bytes,
            file_name="Consolidated_ASIN_Keywords.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        # åˆ›å»ºä¸‹æ‹‰é€‰æ‹©å™¨
        asin_options = ["åˆå¹¶åæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"] + sorted(list(consolidated_df['ASIN'].unique()))
        choice = st.selectbox("è¯·é€‰æ‹©è¦æŸ¥çœ‹çš„è§†å›¾:", asin_options)

        if choice == "åˆå¹¶åæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯":
            display_key_metrics(consolidated_df, is_consolidated=True) # todo:æ–°å»ºç‰¹å®šå‡½æ•°
            plot_asin_traffic_contribution(consolidated_df)
            plot_keyword_traffic(consolidated_df)
            # å±•ç¤ºæœç´¢å’Œè´­ä¹°å æ¯”
            plot_search_volume_and_purchases(consolidated_df)
            plot_keyword_analysis(consolidated_df)  # æ–°å¢çš„ç»¼åˆåˆ†æ
            st.subheader("èšåˆåç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡ (Word Frequency)")
            display_aggregated_word_frequency(consolidated_df)
            display_raw_data(consolidated_df, title="åˆå¹¶åçš„æ•°æ®è¡¨ (Consolidated Data)")
        else:
            # æ ¹æ®é€‰æ‹©çš„ASINç­›é€‰åŸå§‹æ•°æ®è¿›è¡Œå±•ç¤º
            # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ä»åˆå¹¶å‰çš„æ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”çš„DataFrame
            selected_asin_df = None
            for sheet_name, df in individual_sheets.items():
                if choice in sheet_name:
                    selected_asin_df = df
                    break

            if selected_asin_df is not None:
                display_key_metrics(selected_asin_df, asin=choice)
                # å±•ç¤ºæµé‡å æ¯”
                plot_keyword_traffic(selected_asin_df)
                # å±•ç¤ºæœç´¢å’Œè´­ä¹°å æ¯”
                plot_search_volume_and_purchases(selected_asin_df)
                plot_keyword_analysis(selected_asin_df)  # æ–°å¢çš„ç»¼åˆåˆ†æ

                # --- è¯é¢‘åˆ†æ ---
                st.subheader("å•ASINç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡ (Word Frequency)")
                display_aggregated_word_frequency(selected_asin_df)

                display_raw_data(selected_asin_df)


if __name__ == "__main__":
    main()

