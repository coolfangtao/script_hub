import streamlit as st
import pandas as pd
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re
from collections import Counter
from typing import Optional, Dict, Tuple, List, Any
from io import BytesIO
# from shared.usage_tracker import track_script_usage
from shared.sidebar import create_common_sidebar # å‡è®¾è¿™ä¸ªå‡½æ•°å­˜åœ¨äºæ‚¨çš„é¡¹ç›®ä¸­
# track_script_usage("ğŸ“ Listingç”Ÿæˆ")
create_common_sidebar()



# ==============================================================================
# 1. é…ç½®ç±» (AppConfig)
# ==============================================================================
class AppConfig:
    """å­˜æ”¾æ‰€æœ‰åº”ç”¨çš„é…ç½®å‚æ•°"""
    PAGE_TITLE = "ASINå…³é”®è¯åˆ†æé¢æ¿"
    PAGE_LAYOUT = "wide"
    TOP_N_KEYWORDS = 20

    # é¢œè‰²é…ç½®
    COLOR_MAP_TRAFFIC = {"è‡ªç„¶æµé‡ç»å¯¹å æ¯”": "#636EFA", "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”": "#EF553B"}
    COLOR_MAP_PURCHASE = {"è´­ä¹°é‡": "#00CC96", "æœªè´­ä¹°é‡": "#FECB52"}
    COLOR_MAP_KEYWORD_TYPES = {
        'æ ¸å¿ƒå¤§è¯ (é«˜æœç´¢é‡+é«˜æµé‡)': '#FF6B6B',
        'æ½œåŠ›è¯ (é«˜æœç´¢é‡+ä½æµé‡)': '#FFD93D',
        'ç²¾å‡†è¯ (ä½æœç´¢é‡+é«˜æµé‡)': '#6BCF77',
        'é•¿å°¾è¯ (ä½æœç´¢é‡+ä½æµé‡)': '#4D96FF'
    }

    # å›¾è¡¨é«˜åº¦
    CHART_HEIGHT = 800

    # è¯äº‘å›¾é…ç½®
    WORDCLOUD_CONFIG = {
        "width": 1600,
        "height": 900,
        "background_color": None,
        "mode": "RGBA",
        "max_font_size": 200,
        "min_font_size": 10,
        "prefer_horizontal": 0.9,
        "collocations": False
    }

    # ç­–ç•¥å»ºè®®æ–‡æ¡ˆ
    STRATEGY_ADVICE = {
        'æ ¸å¿ƒå¤§è¯ (é«˜æœç´¢é‡+é«˜æµé‡)': "âœ… **ä¿æŒä¼˜åŠ¿**: è¿™äº›æ˜¯æ‚¨çš„æ ¸å¿ƒå…³é”®è¯ï¼Œç»§ç»­ä¿æŒä¼˜åŒ–ï¼Œè€ƒè™‘å¢åŠ ç›¸å…³é•¿å°¾è¯",
        'æ½œåŠ›è¯ (é«˜æœç´¢é‡+ä½æµé‡)': "ğŸš€ **é‡ç‚¹çªç ´**: é«˜æœç´¢é‡ä½†æµé‡ä½ï¼Œéœ€è¦ä¼˜åŒ–é¡µé¢å†…å®¹ã€æé«˜æ’åæˆ–å¢åŠ å¹¿å‘ŠæŠ•å…¥",
        'ç²¾å‡†è¯ (ä½æœç´¢é‡+é«˜æµé‡)': "ğŸ¯ **ç²¾å‡†ç»´æŠ¤**: è½¬åŒ–ç‡é«˜ï¼Œç»´æŠ¤ç°æœ‰æ’åï¼Œå¯é€‚å½“æ‹“å±•ç›¸å…³è¯",
        'é•¿å°¾è¯ (ä½æœç´¢é‡+ä½æµé‡)': "ğŸ“ˆ **é€‰æ‹©æ€§ä¼˜åŒ–**: ç«äº‰è¾ƒå°ï¼Œå¯ä½œä¸ºè¡¥å……ï¼Œé‡ç‚¹å…³æ³¨æœ‰è½¬åŒ–æ½œåŠ›çš„è¯"
    }


# ==============================================================================
# æ•°æ®å¤„ç†æ ¸å¿ƒå‡½æ•° (ä¿æŒç‹¬ç«‹ï¼Œä¾¿äºæµ‹è¯•å’Œå¤ç”¨)
# ==============================================================================

def parse_filename(filename: str) -> Optional[Dict[str, str]]:
    """ä»æ ‡å‡†æ–‡ä»¶åä¸­è§£æå‡ºASINç­‰ä¿¡æ¯ã€‚"""
    pattern = re.compile(r'ReverseASIN-(.*?)-([A-Z0-9]{10})\(.*\)-\d+')
    match = pattern.match(filename)
    if match:
        return {'country': match.group(1), 'asin': match.group(2)}
    return None


def process_uploaded_file(uploaded_file) -> Optional[Tuple[str, pd.DataFrame, pd.DataFrame]]:
    """å¤„ç†å•ä¸ªä¸Šä¼ çš„Excelæ–‡ä»¶ï¼Œè¿”å› (sheet_name, åŸå§‹df, å¸¦ASINåˆ—çš„df)"""
    try:
        file_info = parse_filename(uploaded_file.name)
        if not file_info:
            st.warning(f"æ–‡ä»¶å '{uploaded_file.name}' æ ¼å¼ä¸ç¬¦åˆè¦æ±‚ï¼Œå·²è·³è¿‡ã€‚")
            return None

        asin = file_info['asin']
        original_df = pd.read_excel(uploaded_file, sheet_name=0, engine='openpyxl')
        xls = pd.ExcelFile(uploaded_file, engine='openpyxl')
        sheet_name = xls.sheet_names[0]

        df_for_consolidation = original_df.copy()
        df_for_consolidation.insert(0, 'ASIN', asin)

        return sheet_name, original_df, df_for_consolidation
    except Exception as e:
        st.error(f"å¤„ç†æ–‡ä»¶ '{uploaded_file.name}' æ—¶å‡ºé”™: {e}")
        return None


def create_excel_file(individual_sheets: Dict[str, pd.DataFrame], consolidated_df: pd.DataFrame) -> BytesIO:
    """åˆ›å»ºåŒ…å«æ€»è¡¨å’Œåˆ†è¡¨çš„Excelæ–‡ä»¶ï¼ˆå†…å­˜ä¸­ï¼‰ã€‚"""
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        consolidated_df.to_excel(writer, sheet_name='æ€»è¡¨-æ‰€æœ‰ASINæ•´åˆ', index=False)
        for sheet_name, df in individual_sheets.items():
            df.to_excel(writer, sheet_name=sheet_name, index=False)
    output.seek(0)
    return output


# ==============================================================================
# 2. UIç•Œé¢ç±» (AppUI)
# ==============================================================================
class AppUI:
    """è´Ÿè´£æ‰€æœ‰Streamlitç•Œé¢çš„æ¸²æŸ“"""

    def __init__(self, config: AppConfig):
        self.config = config

    def setup_page(self):
        """è®¾ç½®é¡µé¢é…ç½®ï¼Œåªåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡"""
        st.set_page_config(layout=self.config.PAGE_LAYOUT, page_title=self.config.PAGE_TITLE)

    def display_header_and_uploader(self):
        """æ˜¾ç¤ºæ ‡é¢˜å’Œæ–‡ä»¶ä¸Šä¼ å™¨"""
        st.title(f"ğŸ“Š {self.config.PAGE_TITLE}")
        uploaded_files = st.file_uploader(
            "è¯·ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªASINåæŸ¥çš„Excelæ–‡ä»¶ (æ–‡ä»¶åéœ€ç¬¦åˆ'ReverseASIN-US-B01N9KSITZ(1584)-20251012.xlsx'æ ¼å¼)",
            type="xlsx",
            accept_multiple_files=True
        )
        return uploaded_files

    def display_key_metrics(self, df: pd.DataFrame, is_consolidated: bool = False, asin: str = ""):
        """å±•ç¤ºå…³é”®æŒ‡æ ‡æ€»è§ˆ"""
        st.subheader("å…³é”®æŒ‡æ ‡æ€»è§ˆ (Key Metrics)")
        total_keywords = df['æµé‡è¯'].nunique()
        total_search_volume = df['æœˆæœç´¢é‡'].sum()
        total_purchases = df['è´­ä¹°é‡'].sum()
        avg_purchase_rate = df['è´­ä¹°ç‡'].mean()

        cols = st.columns(4)
        cols[0].metric("å…³é”®è¯æ€»æ•°", f"{total_keywords:,}")
        cols[1].metric("æœˆæœç´¢æ€»é‡", f"{int(total_search_volume):,}")
        cols[2].metric("æœˆè´­ä¹°æ€»é‡", f"{int(total_purchases):,}")
        cols[3].metric("å¹³å‡è´­ä¹°ç‡", f"{avg_purchase_rate:.2%}")

        if is_consolidated:
            total_asins = df['ASIN'].nunique()
            st.info(f"å½“å‰æ•°æ®ä¸º **{total_asins}** ä¸ªASINçš„åˆå¹¶åˆ†æç»“æœã€‚")
        else:
            st.info(f"å½“å‰æ•°æ®ä¸ºå•ä¸ªASIN **{asin}** çš„åˆ†æç»“æœã€‚")

    def display_keyword_traffic_chart(self, df: pd.DataFrame):
        """å±•ç¤ºå…³é”®è¯æµé‡çš„å †å æ¡å½¢å›¾"""
        st.subheader("å…³é”®è¯æµé‡ (Keyword Traffic)")
        df_filtered = df[df['æµé‡å æ¯”'] > 0].copy()
        if df_filtered.empty:
            st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æµé‡æ•°æ®å¯ä¾›å±•ç¤ºã€‚")
            return

        has_asin = 'ASIN' in df_filtered.columns
        df_filtered["è‡ªç„¶æµé‡ç»å¯¹å æ¯”"] = df_filtered["æµé‡å æ¯”"] * df_filtered["è‡ªç„¶æµé‡å æ¯”"]
        df_filtered["å¹¿å‘Šæµé‡ç»å¯¹å æ¯”"] = df_filtered["æµé‡å æ¯”"] * df_filtered["å¹¿å‘Šæµé‡å æ¯”"]

        agg_config = {"æµé‡å æ¯”": "sum", "è‡ªç„¶æµé‡ç»å¯¹å æ¯”": "sum", "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”": "sum"}
        if has_asin:
            agg_config["ASIN"] = "nunique"

        aggregated_df = df_filtered.groupby("æµé‡è¯").agg(agg_config).reset_index()
        if has_asin:
            aggregated_df = aggregated_df.rename(columns={"ASIN": "æ¶‰åŠASINæ•°é‡"})

        top_df = aggregated_df.sort_values(by="æµé‡å æ¯”", ascending=False).head(self.config.TOP_N_KEYWORDS)
        title_suffix = " (å¤šASINæ±‡æ€»)" if has_asin else ""
        chart_title = f"Top {self.config.TOP_N_KEYWORDS} å…³é”®è¯æµé‡åˆ†å¸ƒ{title_suffix}"

        plot_df = top_df.melt(id_vars=["æµé‡è¯"], value_vars=["è‡ªç„¶æµé‡ç»å¯¹å æ¯”", "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”"], var_name="æµé‡ç±»å‹", value_name="å æ¯”")

        hover_data = {}
        if has_asin:
            asin_count_map = top_df.set_index('æµé‡è¯')['æ¶‰åŠASINæ•°é‡'].to_dict()
            plot_df['æ¶‰åŠASINæ•°é‡'] = plot_df['æµé‡è¯'].map(asin_count_map)
            hover_data = {'æ¶‰åŠASINæ•°é‡': True}

        fig = px.bar(plot_df, y="æµé‡è¯", x="å æ¯”", color="æµé‡ç±»å‹", orientation='h', title=chart_title,
                     labels={"æµé‡è¯": "å…³é”®è¯", "å æ¯”": "æµé‡å æ¯”"}, color_discrete_map=self.config.COLOR_MAP_TRAFFIC,
                     text="å æ¯”", hover_data=hover_data)

        fig.update_traces(texttemplate='%{text:.2%}', textposition='inside', insidetextanchor='middle')

        for _, row in top_df.iterrows():
            annotation_text = f"{row['æµé‡å æ¯”']:.2%}"
            if has_asin:
                annotation_text += f" ({row['æ¶‰åŠASINæ•°é‡']}ä¸ªASIN)"
            fig.add_annotation(x=row['æµé‡å æ¯”'], y=row['æµé‡è¯'], text=annotation_text, showarrow=False,
                               xanchor='left', xshift=10, font=dict(color='green', size=12, weight='bold'))

        fig.update_layout(xaxis_title="æµé‡å æ¯”", yaxis_title="å…³é”®è¯", yaxis={'categoryorder': 'total ascending'},
                          height=self.config.CHART_HEIGHT, legend_title_text='æµé‡ç±»å‹', xaxis=dict(tickformat=".2%"))
        st.plotly_chart(fig, use_container_width=True)

    def display_search_purchase_chart(self, df: pd.DataFrame):
        """å±•ç¤ºå…³é”®è¯æœç´¢é‡å’Œè´­ä¹°é‡çš„å †å æ¡å½¢å›¾"""
        st.subheader("å…³é”®è¯æœç´¢é‡å’Œè´­ä¹°é‡ (Search Volume and Purchases)")
        df_filtered = df[df['æœˆæœç´¢é‡'] > 0].copy()
        if df_filtered.empty:
            st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æœˆæœç´¢é‡æ•°æ®å¯ä¾›å±•ç¤ºã€‚")
            return

        has_asin = 'ASIN' in df_filtered.columns
        if has_asin:
            aggregated_df = df_filtered.groupby("æµé‡è¯").agg({
                "æœˆæœç´¢é‡": "sum", "è´­ä¹°é‡": "sum", "ASIN": "nunique"
            }).reset_index().rename(columns={"ASIN": "æ¶‰åŠASINæ•°é‡"})
            aggregated_df["è´­ä¹°ç‡"] = aggregated_df["è´­ä¹°é‡"] / aggregated_df["æœˆæœç´¢é‡"]
            top_df = aggregated_df.sort_values(by="æœˆæœç´¢é‡", ascending=False).head(self.config.TOP_N_KEYWORDS)
        else:
            top_df = df_filtered.sort_values(by="æœˆæœç´¢é‡", ascending=False).head(self.config.TOP_N_KEYWORDS).copy()

        top_df["æœªè´­ä¹°é‡"] = top_df["æœˆæœç´¢é‡"] - top_df["è´­ä¹°é‡"]
        title_suffix = " (å¤šASINæ±‡æ€»)" if has_asin else ""
        chart_title = f"Top {self.config.TOP_N_KEYWORDS} å…³é”®è¯æœç´¢é‡ä¸è´­ä¹°é‡{title_suffix}"
        plot_df = top_df.melt(id_vars=["æµé‡è¯", "è´­ä¹°ç‡"], value_vars=["è´­ä¹°é‡", "æœªè´­ä¹°é‡"], var_name="ç±»å‹", value_name="æ•°é‡")

        hover_data = {}
        if has_asin:
            asin_count_map = top_df.set_index('æµé‡è¯')['æ¶‰åŠASINæ•°é‡'].to_dict()
            plot_df['æ¶‰åŠASINæ•°é‡'] = plot_df['æµé‡è¯'].map(asin_count_map)
            hover_data = {'æ¶‰åŠASINæ•°é‡': True}

        fig = px.bar(plot_df, y="æµé‡è¯", x="æ•°é‡", color="ç±»å‹", orientation='h', title=chart_title,
                     labels={"æµé‡è¯": "å…³é”®è¯", "æ•°é‡": "æœˆæœç´¢é‡"}, color_discrete_map=self.config.COLOR_MAP_PURCHASE,
                     hover_data=hover_data)

        annotations = []
        for _, row in top_df.iterrows():
            purchase_rate_text = f"è´­ä¹°ç‡: {row['è´­ä¹°ç‡']:.2%}"
            if has_asin:
                purchase_rate_text += f" ({row['æ¶‰åŠASINæ•°é‡']}ä¸ªASIN)"
            annotations.append(dict(x=row['æœˆæœç´¢é‡'] * 0.98, y=row['æµé‡è¯'], text=purchase_rate_text,
                                    showarrow=False, font=dict(color="black", size=10), xanchor='right'))
            annotations.append(dict(x=row['æœˆæœç´¢é‡'], y=row['æµé‡è¯'], text=f"{row['æœˆæœç´¢é‡']:,}",
                                    showarrow=False, xanchor='left', xshift=10,
                                    font=dict(color='green', size=12, weight='bold')))

        fig.update_layout(annotations=annotations, xaxis_title="æœˆæœç´¢é‡", yaxis_title="å…³é”®è¯",
                          yaxis={'categoryorder': 'total ascending'}, height=self.config.CHART_HEIGHT,
                          legend_title_text='ç±»å‹')
        st.plotly_chart(fig, use_container_width=True)

    def display_keyword_analysis_section(self, df: pd.DataFrame):
        """æ˜¾ç¤ºå®Œæ•´çš„å…³é”®è¯ç»¼åˆåˆ†ææ¨¡å—"""
        st.subheader("å…³é”®è¯ç»¼åˆåˆ†æ (Keyword Analysis)")
        df_filtered = df[(df['æœˆæœç´¢é‡'] > 0) & (df['æµé‡å æ¯”'] > 0)].copy()
        if df_filtered.empty:
            st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æœç´¢é‡å’Œæµé‡æ•°æ®å¯ä¾›åˆ†æã€‚")
            return

        has_asin = 'ASIN' in df_filtered.columns
        if has_asin:
            asin_info = df_filtered.groupby('æµé‡è¯')['ASIN'].apply(list).reset_index(name='æ¶‰åŠASINåˆ—è¡¨')
            aggregated_df = df_filtered.groupby("æµé‡è¯").agg({
                "æœˆæœç´¢é‡": "sum", "æµé‡å æ¯”": "sum", "è´­ä¹°ç‡": "mean",
                "è‡ªç„¶æµé‡å æ¯”": "mean", "å¹¿å‘Šæµé‡å æ¯”": "mean", "ASIN": "nunique"
            }).reset_index().rename(columns={"ASIN": "æ¶‰åŠASINæ•°é‡"})
            df_filtered = aggregated_df.merge(asin_info, on='æµé‡è¯', how='left')
            st.info("ğŸ“Š å½“å‰æ˜¾ç¤ºå¤šASINæ±‡æ€»æ•°æ®ï¼Œå·²æŒ‰å…³é”®è¯èšåˆ")

        df_filtered['æ€»æµé‡è´¡çŒ®'] = df_filtered['æµé‡å æ¯”'] * 100
        search_median = df_filtered['æœˆæœç´¢é‡'].median()
        traffic_median = df_filtered['æ€»æµé‡è´¡çŒ®'].median()

        def classify_keyword(row):
            if row['æœˆæœç´¢é‡'] > search_median and row['æ€»æµé‡è´¡çŒ®'] > traffic_median:
                return 'æ ¸å¿ƒå¤§è¯ (é«˜æœç´¢é‡+é«˜æµé‡)'
            elif row['æœˆæœç´¢é‡'] > search_median and row['æ€»æµé‡è´¡çŒ®'] <= traffic_median:
                return 'æ½œåŠ›è¯ (é«˜æœç´¢é‡+ä½æµé‡)'
            elif row['æœˆæœç´¢é‡'] <= search_median and row['æ€»æµé‡è´¡çŒ®'] > traffic_median:
                return 'ç²¾å‡†è¯ (ä½æœç´¢é‡+é«˜æµé‡)'
            else:
                return 'é•¿å°¾è¯ (ä½æœç´¢é‡+ä½æµé‡)'

        df_filtered['å…³é”®è¯ç±»å‹'] = df_filtered.apply(classify_keyword, axis=1)

        # ... (æ­¤å¤„çœç•¥äº†å›¾è¡¨åˆ›å»ºå’Œæ˜¾ç¤ºé€»è¾‘ï¼Œä¸åŸä»£ç ç›¸åŒï¼Œç›´æ¥å¤ç”¨)
        # æ‚¨å¯ä»¥å°†åŸä»£ç ä¸­ plot_keyword_analysis å‡½æ•°çš„å›¾è¡¨éƒ¨åˆ†ç²˜è´´åˆ°è¿™é‡Œ
        # ä¸ºäº†ç®€æ´ï¼Œæ­¤å¤„ç›´æ¥è°ƒç”¨ä¸€ä¸ªå†…éƒ¨æ–¹æ³•
        self._plot_keyword_analysis_scatter(df_filtered, has_asin, search_median, traffic_median)

        # æ˜¾ç¤ºç»Ÿè®¡ã€æ•°æ®è¡¨æ ¼å’Œç­–ç•¥å»ºè®®
        self._display_keyword_analysis_details(df_filtered, has_asin, search_median, traffic_median)

    def _plot_keyword_analysis_scatter(self, df_filtered, has_asin, search_median, traffic_median):
        """å†…éƒ¨æ–¹æ³•ï¼šç»˜åˆ¶å…³é”®è¯åˆ†ææ•£ç‚¹å›¾"""
        title_suffix = " (å¤šASINæ±‡æ€»)" if has_asin else ""
        chart_title = f'å…³é”®è¯æœç´¢é‡ vs æµé‡å æ¯”åˆ†æ{title_suffix}'

        hover_template = "..."  # (ä»åŸä»£ç å¤åˆ¶)
        hover_data = ['æµé‡è¯', 'è´­ä¹°ç‡', 'è‡ªç„¶æµé‡å æ¯”', 'å¹¿å‘Šæµé‡å æ¯”']
        if has_asin:
            df_filtered['ASINæ˜¾ç¤º'] = df_filtered['æ¶‰åŠASINåˆ—è¡¨'].apply(
                lambda x: ', '.join(x[:5]) + ('...' if len(x) > 5 else ''))
            hover_template += "..."  # (ä»åŸä»£ç å¤åˆ¶)
            hover_data.extend(['æ¶‰åŠASINæ•°é‡', 'ASINæ˜¾ç¤º'])

        # å®Œæ•´çš„ hover_template
        hover_template = (
            "<span style='font-size: 16px; font-weight: bold; color: green'>ğŸ“Œ %{customdata[0]}</span><br><br>"
            "<b>æœç´¢é‡:</b> %{x:,}<br>"
            "<b>æµé‡å æ¯”:</b> %{y:.2f}%<br>"
            "<b>è´­ä¹°ç‡:</b> %{customdata[1]:.2%}<br>"
            "<b>è‡ªç„¶æµé‡:</b> %{customdata[2]:.2%}<br>"
            "<b>å¹¿å‘Šæµé‡:</b> %{customdata[3]:.2%}"
        )
        if has_asin:
            hover_template += "<br><b>æ¶‰åŠASINæ•°é‡:</b> %{customdata[4]}<br>"
            hover_template += "<b>æ¶‰åŠASIN:</b> %{customdata[5]}"
        hover_template += "<extra></extra>"

        fig = px.scatter(df_filtered, x='æœˆæœç´¢é‡', y='æ€»æµé‡è´¡çŒ®', color='å…³é”®è¯ç±»å‹', title=chart_title,
                         labels={'æœˆæœç´¢é‡': 'æœˆæœç´¢é‡', 'æ€»æµé‡è´¡çŒ®': 'æµé‡å æ¯” (%)', 'å…³é”®è¯ç±»å‹': 'å…³é”®è¯ç±»å‹'},
                         color_discrete_map=self.config.COLOR_MAP_KEYWORD_TYPES)
        fig.update_traces(marker=dict(size=8), hovertemplate=hover_template, customdata=df_filtered[hover_data])
        fig.add_hline(y=traffic_median, line_dash="dash", line_color="red",
                      annotation_text=f"æµé‡ä¸­ä½æ•°: {traffic_median:.2f}%")
        fig.add_vline(x=search_median, line_dash="dash", line_color="red",
                      annotation_text=f"æœç´¢é‡ä¸­ä½æ•°: {search_median:,.0f}")
        fig.update_layout(height=600)
        st.plotly_chart(fig, use_container_width=True)

    def _display_keyword_analysis_details(self, df_filtered, has_asin, search_median, traffic_median):
        """å†…éƒ¨æ–¹æ³•ï¼šæ˜¾ç¤ºå…³é”®è¯åˆ†æçš„ç»Ÿè®¡æ•°æ®ã€è¡¨æ ¼å’Œå»ºè®®"""
        col1, col2 = st.columns(2)
        with col1:
            st.write("### ğŸ“Š åˆ†ç±»ç»Ÿè®¡")
            type_stats = df_filtered['å…³é”®è¯ç±»å‹'].value_counts()
            for type_name, count in type_stats.items():
                st.write(f"**{type_name}**: {count}ä¸ª ({count / len(df_filtered):.1%})")
        with col2:
            st.write("### ğŸ“ˆ å…³é”®æŒ‡æ ‡")
            c1, c2 = st.columns(2)
            c1.metric("æ€»å…³é”®è¯æ•°", len(df_filtered))
            c1.metric("æœç´¢é‡ä¸­ä½æ•°", f"{search_median:,.0f}")
            c2.metric("æµé‡å æ¯”ä¸­ä½æ•°", f"{traffic_median:.2f}%")
            if has_asin:
                c2.metric("æ€»æ¶‰åŠASINæ•°", df_filtered['æ¶‰åŠASINæ•°é‡'].max())  # å‡è®¾èšåˆåæ•°é‡ä¸€è‡´

        st.write("### ğŸ“‹ è¯¦ç»†æ•°æ®")
        selected_types = st.multiselect("é€‰æ‹©è¦æ˜¾ç¤ºçš„å…³é”®è¯ç±»å‹:", options=df_filtered['å…³é”®è¯ç±»å‹'].unique(),
                                        default=df_filtered['å…³é”®è¯ç±»å‹'].unique())
        display_df = df_filtered[df_filtered['å…³é”®è¯ç±»å‹'].isin(selected_types)]
        # ... (æ­¤å¤„çœç•¥äº†è¡¨æ ¼çš„åˆ›å»ºå’Œæ ¼å¼åŒ–é€»è¾‘ï¼Œä¸åŸä»£ç ç›¸åŒ)
        st.dataframe(display_df, use_container_width=True)  # ç®€åŒ–æ˜¾ç¤ºï¼Œå¯æŒ‰åŸä»£ç æ ¼å¼åŒ–

        st.write("### ğŸ’¡ ç­–ç•¥å»ºè®®")
        for keyword_type in selected_types:
            if keyword_type in self.config.STRATEGY_ADVICE:
                st.info(f"**{keyword_type}**: {self.config.STRATEGY_ADVICE[keyword_type]}")

    def display_word_frequency_section(self, df: pd.DataFrame, title: str):
        """æ˜¾ç¤ºå•è¯é¢‘ç‡åˆ†ææ¨¡å—ï¼ˆè¯äº‘+è¡¨æ ¼ï¼‰"""
        st.subheader(title)
        text = ' '.join(df['æµé‡è¯'].dropna())
        words = re.findall(r'\b\w+\b', text.lower())
        word_counts = Counter(words)

        if not word_counts:
            st.warning("æ²¡æœ‰å¯ç”¨äºè¯é¢‘åˆ†æçš„å…³é”®è¯ã€‚")
            return

        # è¯äº‘å›¾
        wordcloud = WordCloud(**self.config.WORDCLOUD_CONFIG).generate_from_frequencies(word_counts)
        fig, ax = plt.subplots(figsize=(12, 9), dpi=300)
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        fig.patch.set_alpha(0)
        ax.patch.set_alpha(0)
        st.pyplot(fig, bbox_inches='tight', pad_inches=0)

        # é¢‘ç‡è¡¨æ ¼
        total_words = sum(word_counts.values())
        freq_df = pd.DataFrame(word_counts.items(), columns=["å•è¯", "å‡ºç°æ¬¡æ•°"])
        freq_df["é¢‘ç‡"] = freq_df["å‡ºç°æ¬¡æ•°"] / total_words
        freq_df = freq_df.sort_values(by="å‡ºç°æ¬¡æ•°", ascending=False)
        st.dataframe(freq_df.head(20).style.format({"é¢‘ç‡": "{:.2%}"}), height=600, use_container_width=True)

    def display_asin_traffic_contribution_chart(self, df: pd.DataFrame):
        """å±•ç¤ºå„ASINæµé‡è´¡çŒ®å¯¹æ¯”çš„æŸ±çŠ¶å›¾"""
        st.subheader("å„ASINæµé‡è´¡çŒ®å¯¹æ¯” (ASIN Traffic Contribution)")
        df_copy = df.copy()
        df_copy['æµé‡è´¡çŒ®å€¼'] = df_copy['æµé‡å æ¯”'] * df_copy['æœˆæœç´¢é‡']
        asin_traffic = df_copy.groupby('ASIN')['æµé‡è´¡çŒ®å€¼'].sum().sort_values(ascending=False).reset_index()

        fig = px.bar(asin_traffic, x='ASIN', y='æµé‡è´¡çŒ®å€¼', title="å„ASINæµé‡è´¡çŒ®å€¼å¯¹æ¯”",
                     labels={'ASIN': 'äº§å“ASIN', 'æµé‡è´¡çŒ®å€¼': 'æµé‡è´¡çŒ®å€¼ (æµé‡å æ¯” * æœˆæœç´¢é‡)'}, text='æµé‡è´¡çŒ®å€¼')
        fig.update_traces(texttemplate='%{text:,.0f}', textposition='outside')
        fig.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)

    def display_raw_data(self, df: pd.DataFrame, title: str):
        """å±•ç¤ºåŸå§‹æ•°æ®è¡¨æ ¼"""
        st.subheader(title)
        st.dataframe(df)

    def display_download_button(self, data: BytesIO, label: str, file_name: str, mime: str):
        """æ˜¾ç¤ºä¸‹è½½æŒ‰é’®"""
        st.download_button(label=label, data=data, file_name=file_name, mime=mime)


# ==============================================================================
# 3. ä¸»åº”ç”¨é€»è¾‘ (main)
# ==============================================================================

def main():
    """åº”ç”¨ä¸»å‡½æ•°"""
    config = AppConfig()
    ui = AppUI(config)

    ui.setup_page()
    # track_script_usage("é¡µé¢åç§°")
    create_common_sidebar()
    uploaded_files = ui.display_header_and_uploader()

    # ------------------ ä¼˜åŒ–åçš„çŠ¶æ€ç®¡ç†é€»è¾‘ ------------------

    # åªæœ‰å½“ç”¨æˆ·ä¸Šä¼ äº†æ–°æ–‡ä»¶æ—¶ï¼Œæ‰è§¦å‘æ•°æ®å¤„ç†å’Œç¼“å­˜æ›´æ–°
    if uploaded_files:
        current_file_names = sorted([f.name for f in uploaded_files])
        # æ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶æ˜¯å¦ä¸ç¼“å­˜ä¸­çš„æ–‡ä»¶ä¸åŒ
        if st.session_state.get('file_names') != current_file_names:
            with st.spinner("æ£€æµ‹åˆ°æ–°æ–‡ä»¶ï¼Œæ­£åœ¨å¤„ç†ä¸­..."):
                individual_sheets = {}
                dfs_for_consolidation = []
                for file in uploaded_files:
                    result = process_uploaded_file(file)
                    if result:
                        sheet_name, original_df, df_for_consolidation = result
                        individual_sheets[sheet_name] = original_df
                        dfs_for_consolidation.append(df_for_consolidation)

                if dfs_for_consolidation:
                    consolidated_df = pd.concat(dfs_for_consolidation, ignore_index=True)
                    st.session_state.processed_data = {
                        "individual": individual_sheets,
                        "consolidated": consolidated_df
                    }
                    st.session_state.file_names = current_file_names
                    st.success(f"æˆåŠŸå¤„ç†å¹¶ç¼“å­˜äº† {len(dfs_for_consolidation)} ä¸ªæ–‡ä»¶ï¼")
                else:
                    # å¦‚æœä¸Šä¼ çš„æ–‡ä»¶éƒ½å¤„ç†å¤±è´¥ï¼Œåˆ™æ¸…ç©ºçŠ¶æ€
                    st.session_state.processed_data = None
                    st.session_state.file_names = None

    # ------------------ æ¸²æŸ“é€»è¾‘ (ä¿æŒä¸å˜) ------------------
    if not st.session_state.get('processed_data'):
        st.info("è¯·ä¸Šä¼ æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")
        return

    # ä» session_state ä¸­è·å–æ•°æ®
    processed_data = st.session_state.processed_data
    individual_sheets = processed_data["individual"]
    consolidated_df = processed_data["consolidated"]
    num_files = len(individual_sheets)

    # æ ¹æ®æ–‡ä»¶æ•°é‡å†³å®šæ˜¾ç¤ºé€»è¾‘
    if num_files == 1:
        # å•æ–‡ä»¶è§†å›¾
        sheet_name = list(individual_sheets.keys())[0]
        df = individual_sheets[sheet_name]
        file_info = parse_filename(sheet_name.replace(" ", "_"))  # å‡è®¾sheetååŒ…å«æ–‡ä»¶å
        asin = file_info['asin'] if file_info else "æœªçŸ¥"

        ui.display_key_metrics(df, asin=asin)
        ui.display_keyword_traffic_chart(df)
        ui.display_search_purchase_chart(df)
        ui.display_keyword_analysis_section(df)
        ui.display_word_frequency_section(df, "å•ASINç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡")
        ui.display_raw_data(df, "åŸå§‹æ•°æ®")

    elif num_files > 1:
        # å¤šæ–‡ä»¶è§†å›¾
        # æä¾›ä¸‹è½½æŒ‰é’®
        excel_bytes = create_excel_file(individual_sheets, consolidated_df)
        ui.display_download_button(
            data=excel_bytes,
            label="ğŸ“¥ ä¸‹è½½åˆå¹¶åçš„Excelæ–‡ä»¶",
            file_name="Consolidated_ASIN_Keywords.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        # åˆ›å»ºè§†å›¾é€‰æ‹©å™¨
        asin_options = ["åˆå¹¶åæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"] + sorted(list(consolidated_df['ASIN'].unique()))
        choice = st.selectbox("è¯·é€‰æ‹©è¦æŸ¥çœ‹çš„è§†å›¾:", asin_options)

        if choice == "åˆå¹¶åæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯":
            df_to_show = consolidated_df
            ui.display_key_metrics(df_to_show, is_consolidated=True)
            ui.display_asin_traffic_contribution_chart(df_to_show)
            ui.display_keyword_traffic_chart(df_to_show)
            ui.display_search_purchase_chart(df_to_show)
            ui.display_keyword_analysis_section(df_to_show)
            ui.display_word_frequency_section(df_to_show, "èšåˆåç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡")
            ui.display_raw_data(df_to_show, "åˆå¹¶åçš„æ•°æ®è¡¨")
        else:
            # å¯»æ‰¾è¢«é€‰ä¸­çš„ASINå¯¹åº”çš„åŸå§‹DataFrame
            selected_df = None
            for sheet_name, df_item in individual_sheets.items():
                if choice in sheet_name:
                    selected_df = df_item
                    break

            if selected_df is not None:
                ui.display_key_metrics(selected_df, asin=choice)
                ui.display_keyword_traffic_chart(selected_df)
                ui.display_search_purchase_chart(selected_df)
                ui.display_keyword_analysis_section(selected_df)
                ui.display_word_frequency_section(selected_df, "å•ASINç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡")
                ui.display_raw_data(selected_df, "åŸå§‹æ•°æ®")


if __name__ == "__main__":
    main()