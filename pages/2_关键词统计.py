import streamlit as st
import pandas as pd
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re
from collections import Counter
from typing import Optional, Dict, Tuple, List
from io import BytesIO
from shared.sidebar import create_common_sidebar # <-- 1. å¯¼å…¥å‡½æ•°
create_common_sidebar() # <-- 2. è°ƒç”¨å‡½æ•°ï¼Œç¡®ä¿æ¯ä¸ªé¡µé¢éƒ½æœ‰ä¾§è¾¹æ 


# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---
def parse_filename(filename: str) -> Optional[Dict[str, str]]:
    """
    ä»æ ‡å‡†æ–‡ä»¶åä¸­è§£æå‡ºASINç­‰ä¿¡æ¯ã€‚
    æ–‡ä»¶åæ ¼å¼: ReverseASIN-US-B01N9KSITZ(1584)-20251012.xlsx
    """
    pattern = re.compile(r'ReverseASIN-(.*?)-([A-Z0-9]{10})\(.*\)-\d+')
    match = pattern.match(filename)
    if match:
        return {'country': match.group(1), 'asin': match.group(2)}
    return None


def process_uploaded_file(uploaded_file) -> Optional[Tuple[str, pd.DataFrame, pd.DataFrame]]:
    """
    å¤„ç†å•ä¸ªä¸Šä¼ çš„Excelæ–‡ä»¶ã€‚
    1. è¯»å–ç¬¬ä¸€ä¸ªsheetã€‚
    2. è§£ææ–‡ä»¶åè·å–ASINã€‚
    3. åˆ›å»ºä¸€ä¸ªæ–°DataFrameï¼Œå¹¶åœ¨ç¬¬ä¸€åˆ—æ·»åŠ ASINä¿¡æ¯ã€‚

    Args:
        uploaded_file: Streamlitä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡ã€‚

    Returns:
        Optional[Tuple[str, pd.DataFrame, pd.DataFrame]]:
        ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«:
        - åŸå§‹sheetçš„åç§°ã€‚
        - åŸå§‹sheetçš„å®Œæ•´DataFrame (ç”¨äºå•ç‹¬å­˜æ”¾)ã€‚
        - æ·»åŠ äº†ASINåˆ—çš„DataFrame (ç”¨äºæ€»è¡¨åˆå¹¶)ã€‚
        å¦‚æœæ–‡ä»¶å¤„ç†å¤±è´¥æˆ–æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ™è¿”å›Noneã€‚
    """
    try:
        # è§£ææ–‡ä»¶å
        file_info = parse_filename(uploaded_file.name)
        if not file_info:
            st.warning(f"æ–‡ä»¶å '{uploaded_file.name}' æ ¼å¼ä¸ç¬¦åˆè¦æ±‚ï¼Œå·²è·³è¿‡ã€‚")
            return None

        asin = file_info['asin']

        # è¯»å–Excelçš„ç¬¬ä¸€ä¸ªsheet
        # `sheet_name=0` è¡¨ç¤ºè¯»å–ç¬¬ä¸€ä¸ªsheet
        # `engine='openpyxl'` æ˜¯ä¸ºäº†æ›´å¥½åœ°å…¼å®¹.xlsxæ–‡ä»¶
        original_df = pd.read_excel(uploaded_file, sheet_name=0, engine='openpyxl')

        # è·å–ç¬¬ä¸€ä¸ªsheetçš„åç§°ï¼Œç”¨äºåœ¨æ–°Excelä¸­åˆ›å»ºåŒåsheet
        xls = pd.ExcelFile(uploaded_file, engine='openpyxl')
        sheet_name = xls.sheet_names[0]

        # å‡†å¤‡ç”¨äºåˆå¹¶åˆ°æ€»è¡¨çš„æ•°æ®
        df_for_consolidation = original_df.copy()
        # åœ¨ç¬¬ä¸€åˆ—æ’å…¥ASINä¿¡æ¯
        df_for_consolidation.insert(0, 'ASIN', asin)

        return sheet_name, original_df, df_for_consolidation

    except Exception as e:
        st.error(f"å¤„ç†æ–‡ä»¶ '{uploaded_file.name}' æ—¶å‡ºé”™: {e}")
        return None


def create_excel_file(individual_sheets: Dict[str, pd.DataFrame], consolidated_df: pd.DataFrame) -> BytesIO:
    """
    å°†å¤„ç†å¥½çš„æ•°æ®å†™å…¥ä¸€ä¸ªæ–°çš„Excelæ–‡ä»¶ï¼ˆåœ¨å†…å­˜ä¸­ï¼‰ã€‚

    Args:
        individual_sheets (Dict[str, pd.DataFrame]): ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯sheetåï¼Œå€¼æ˜¯å¯¹åº”çš„åŸå§‹DataFrameã€‚
        consolidated_df (pd.DataFrame): æ•´åˆäº†æ‰€æœ‰ASINä¿¡æ¯çš„æ€»è¡¨DataFrameã€‚

    Returns:
        BytesIO: åŒ…å«æ–°Excelæ–‡ä»¶å†…å®¹çš„äºŒè¿›åˆ¶æµå¯¹è±¡ã€‚
    """
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        # 1. å†™å…¥æ€»è¡¨ï¼Œå¹¶å°†å…¶æ”¾åœ¨ç¬¬ä¸€ä¸ª
        consolidated_df.to_excel(writer, sheet_name='æ€»è¡¨-æ‰€æœ‰ASINæ•´åˆ', index=False)

        # 2. å†™å…¥æ¯ä¸ªç‹¬ç«‹çš„ASIN sheet
        for sheet_name, df in individual_sheets.items():
            df.to_excel(writer, sheet_name=sheet_name, index=False)

    # é‡ç½®æŒ‡é’ˆåˆ°æ–‡ä»¶å¼€å¤´ï¼Œä»¥ä¾¿st.download_buttonå¯ä»¥è¯»å–å®ƒ
    output.seek(0)
    return output


# --- æ•°æ®åˆ†æä¸å¯è§†åŒ–å‡½æ•° ---
def display_key_metrics(df: pd.DataFrame, is_consolidated: bool = False, asin: str = ""):
    """å±•ç¤ºå…³é”®æŒ‡æ ‡æ€»è§ˆã€‚"""
    st.subheader("å…³é”®æŒ‡æ ‡æ€»è§ˆ (Key Metrics)")

    total_keywords = df['æµé‡è¯'].nunique()
    total_search_volume = df['æœˆæœç´¢é‡'].sum()
    total_purchases = df['è´­ä¹°é‡'].sum()
    avg_purchase_rate = df['è´­ä¹°ç‡'].mean()

    cols = st.columns(4)
    with cols[0]:
        st.metric("å…³é”®è¯æ€»æ•°", f"{total_keywords:,}")
    with cols[1]:
        st.metric("æœˆæœç´¢æ€»é‡", f"{int(total_search_volume):,}")
    with cols[2]:
        st.metric("æœˆè´­ä¹°æ€»é‡", f"{int(total_purchases):,}")
    with cols[3]:
        st.metric("å¹³å‡è´­ä¹°ç‡", f"{avg_purchase_rate:.2%}")

    if is_consolidated:
        total_asins = df['ASIN'].nunique()
        st.info(f"å½“å‰æ•°æ®ä¸º **{total_asins}** ä¸ªASINçš„åˆå¹¶åˆ†æç»“æœã€‚")
    else:
        st.info(f"å½“å‰æ•°æ®ä¸ºå•ä¸ªASIN **{asin}** çš„åˆ†æç»“æœã€‚")


def plot_keyword_traffic(df):
    """
    å±•ç¤ºå…³é”®è¯æµé‡çš„å †å æ¡å½¢å›¾ã€‚

    Args:
        df (pd.DataFrame): åŒ…å«å…³é”®è¯æ•°æ®çš„DataFrameã€‚
    """
    st.subheader("å…³é”®è¯æµé‡ (Keyword Traffic)")

    # é€‰å–æµé‡å æ¯”æœ€é«˜çš„å‰20ä¸ªå…³é”®è¯
    top_20_traffic = df.sort_values(by="æµé‡å æ¯”", ascending=False).head(20)

    # è®¡ç®—è‡ªç„¶æµé‡å’Œå¹¿å‘Šæµé‡çš„ç»å¯¹å æ¯”
    top_20_traffic["è‡ªç„¶æµé‡ç»å¯¹å æ¯”"] = top_20_traffic["æµé‡å æ¯”"] * top_20_traffic["è‡ªç„¶æµé‡å æ¯”"]
    top_20_traffic["å¹¿å‘Šæµé‡ç»å¯¹å æ¯”"] = top_20_traffic["æµé‡å æ¯”"] * top_20_traffic["å¹¿å‘Šæµé‡å æ¯”"]

    # èåŒ–DataFrameä»¥é€‚é…Plotlyçš„æ ¼å¼
    plot_df = top_20_traffic.melt(
        id_vars=["æµé‡è¯"],
        value_vars=["è‡ªç„¶æµé‡ç»å¯¹å æ¯”", "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”"],
        var_name="æµé‡ç±»å‹",
        value_name="å æ¯”"
    )

    # ç»˜åˆ¶å›¾è¡¨
    fig = px.bar(
        plot_df,
        y="æµé‡è¯",
        x="å æ¯”",
        color="æµé‡ç±»å‹",
        orientation='h',
        title="Top 20 å…³é”®è¯æµé‡åˆ†å¸ƒ",
        labels={"æµé‡è¯": "å…³é”®è¯", "å æ¯”": "æµé‡å æ¯”"},
        color_discrete_map={
            "è‡ªç„¶æµé‡ç»å¯¹å æ¯”": "#636EFA",
            "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”": "#EF553B"
        },
        text="å æ¯”"  # æ·»åŠ æ–‡æœ¬æ˜¾ç¤º
    )

    # æ ¼å¼åŒ–æ–‡æœ¬æ˜¾ç¤º
    fig.update_traces(
        texttemplate='%{text:.1%}',  # æ˜¾ç¤ºä¸ºç™¾åˆ†æ¯”æ ¼å¼ï¼Œä¿ç•™1ä½å°æ•°
        textposition='inside',  # æ–‡æœ¬æ˜¾ç¤ºåœ¨æŸ±å­å†…éƒ¨
        insidetextanchor='middle'  # æ–‡æœ¬åœ¨æŸ±å­ä¸­é—´
    )

    fig.update_layout(
        xaxis_title="æµé‡å æ¯”",
        yaxis_title="å…³é”®è¯",
        yaxis={'categoryorder': 'total ascending'},
        height=800,
        legend_title_text='æµé‡ç±»å‹',
        xaxis=dict(
            tickformat=".1%",  # xè½´åˆ»åº¦æ˜¾ç¤ºä¸ºç™¾åˆ†æ¯”
            range=[0, top_20_traffic["æµé‡å æ¯”"].max() * 1.1]  # è°ƒæ•´xè½´èŒƒå›´
        )
    )

    st.plotly_chart(fig, use_container_width=True)


def calculate_word_frequency(df: pd.DataFrame) -> Tuple[Dict[str, int], pd.DataFrame]:
    """ä»æµé‡è¯åˆ—è®¡ç®—å•è¯é¢‘ç‡ã€‚"""
    words = df['æµé‡è¯'].dropna().str.lower().str.split().sum()
    word_counts = Counter(words)
    total_words = sum(word_counts.values())

    freq_df = pd.DataFrame(word_counts.items(), columns=['å•è¯', 'å‡ºç°æ¬¡æ•°'])
    freq_df['é¢‘ç‡'] = freq_df['å‡ºç°æ¬¡æ•°'] / total_words
    freq_df = freq_df.sort_values(by='å‡ºç°æ¬¡æ•°', ascending=False).reset_index(drop=True)

    return word_counts, freq_df


def display_word_frequency_analysis(df: pd.DataFrame):
    """å±•ç¤ºå•è¯é¢‘ç‡è¯äº‘å’Œè¡¨æ ¼ã€‚"""
    st.subheader("ç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡ (Word Frequency)")
    word_counts, freq_df = calculate_word_frequency(df)

    if not word_counts:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æµé‡è¯æ¥ç”Ÿæˆå•è¯é¢‘ç‡åˆ†æã€‚")
        return

    col1, col2 = st.columns([2, 1])
    with col1:
        st.write("å•è¯è¯äº‘")
        wordcloud = WordCloud(
            width=800, height=500, background_color='white',
            max_words=150, collocations=False
        ).generate_from_frequencies(word_counts)
        fig, ax = plt.subplots()
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    with col2:
        st.write("å•è¯é¢‘ç‡ç»Ÿè®¡ (Top 20)")
        st.dataframe(
            freq_df.head(20).style.format({"é¢‘ç‡": "{:.2%}"}),
            height=500, use_container_width=True
        )

def display_word_cloud(word_counts):
    st.write("å•è¯è¯äº‘")
    wordcloud = WordCloud(
        width=1600,  # å¢å¤§ç”»å¸ƒå°ºå¯¸
        height=900,
        background_color=None,
        mode="RGBA",
        # max_words=200,  # é™åˆ¶å•è¯æ•°é‡
        max_font_size=200,  # è®¾ç½®æœ€å¤§å­—ä½“
        min_font_size=10,  # è®¾ç½®æœ€å°å­—ä½“
        prefer_horizontal=0.9,  # è°ƒæ•´æ°´å¹³æ˜¾ç¤ºåå¥½
        # relative_scaling=0.5,  # è°ƒæ•´ç¼©æ”¾æ¯”ä¾‹
        collocations=False  # ç¦ç”¨è¯ç»„
    ).generate_from_frequencies(word_counts)

    # åˆ›å»ºå›¾å½¢æ—¶è®¾ç½®æ›´é«˜çš„DPI
    fig, ax = plt.subplots(figsize=(12, 9), dpi=300)
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')

    # è®¾ç½®å›¾å½¢èƒŒæ™¯é€æ˜
    fig.patch.set_alpha(0)
    ax.patch.set_alpha(0)

    # ä¿å­˜ä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒ
    st.pyplot(fig, bbox_inches='tight', pad_inches=0, dpi=300)


def display_frequency_table(freq_df):
    """
    å±•ç¤ºæ ¼å¼åŒ–åçš„å•è¯é¢‘ç‡ç»Ÿè®¡è¡¨æ ¼ã€‚

    Args:
        freq_df (pd.DataFrame): åŒ…å«å•è¯ã€å‡ºç°æ¬¡æ•°å’Œé¢‘ç‡çš„DataFrameã€‚
    """
    st.write("å•è¯é¢‘ç‡ç»Ÿè®¡")
    st.dataframe(
        freq_df.head(20).style.format({"é¢‘ç‡": "{:.2%}"}),
        height=600,
        use_container_width=True
    )

def plot_search_volume_and_purchases(df: pd.DataFrame):
    """å±•ç¤ºå…³é”®è¯æœç´¢é‡å’Œè´­ä¹°é‡çš„å †å æ¡å½¢å›¾ã€‚"""
    st.subheader("å…³é”®è¯æœç´¢é‡å’Œè´­ä¹°é‡ (Search Volume and Purchases)")

    df_filtered = df[df['æœˆæœç´¢é‡'] > 0].copy()
    if df_filtered.empty:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æœˆæœç´¢é‡æ•°æ®å¯ä¾›å±•ç¤ºã€‚")
        return

    top_20_search = df_filtered.sort_values(by="æœˆæœç´¢é‡", ascending=False).head(20)
    top_20_search["æœªè´­ä¹°é‡"] = top_20_search["æœˆæœç´¢é‡"] - top_20_search["è´­ä¹°é‡"]

    plot_df = top_20_search.melt(
        id_vars=["æµé‡è¯", "è´­ä¹°ç‡"],
        value_vars=["è´­ä¹°é‡", "æœªè´­ä¹°é‡"],
        var_name="ç±»å‹", value_name="æ•°é‡"
    )

    fig = px.bar(
        plot_df, y="æµé‡è¯", x="æ•°é‡", color="ç±»å‹", orientation='h',
        title="Top 20 å…³é”®è¯æœç´¢é‡ä¸è´­ä¹°é‡",
        labels={"æµé‡è¯": "å…³é”®è¯", "æ•°é‡": "æœˆæœç´¢é‡"},
        color_discrete_map={"è´­ä¹°é‡": "#00CC96", "æœªè´­ä¹°é‡": "#FECB52"}
    )

    # ä¸ºæ¯ä¸ªæ¡å½¢æ·»åŠ è´­ä¹°ç‡æ ‡æ³¨
    annotations = []
    for _, row in top_20_search.iterrows():
        annotations.append(dict(
            x=row['æœˆæœç´¢é‡'] * 0.98, y=row['æµé‡è¯'],
            text=f"è´­ä¹°ç‡: {row['è´­ä¹°ç‡']:.2%}",
            showarrow=False, font=dict(color="black", size=10),
            xanchor='right'
        ))
    fig.update_layout(annotations=annotations)

    fig.update_layout(
        xaxis_title="æœˆæœç´¢é‡", yaxis_title="å…³é”®è¯",
        yaxis={'categoryorder': 'total ascending'}, height=800,
        legend_title_text='ç±»å‹'
    )
    st.plotly_chart(fig, use_container_width=True)


def display_raw_data(df: pd.DataFrame, title: str = "åŸå§‹æ•°æ® (Raw Data)"):
    """å±•ç¤ºåŸå§‹æ•°æ®è¡¨æ ¼ã€‚"""
    st.subheader(title)
    st.dataframe(df)


# --- åˆå¹¶æ–‡ä»¶åçš„ä¸“å±å¯è§†åŒ–å‡½æ•° ---

def plot_asin_traffic_contribution(df: pd.DataFrame):
    """å±•ç¤ºå„ASINæµé‡è´¡çŒ®å¯¹æ¯”çš„æŸ±çŠ¶å›¾ã€‚"""
    st.subheader("å„ASINæµé‡è´¡çŒ®å¯¹æ¯” (ASIN Traffic Contribution)")

    # è®¡ç®—æ¯ä¸ªASINçš„æ€»æµé‡è´¡çŒ®å€¼ï¼ˆæµé‡å æ¯” * æœˆæœç´¢é‡ï¼‰
    df['æµé‡è´¡çŒ®å€¼'] = df['æµé‡å æ¯”'] * df['æœˆæœç´¢é‡']
    asin_traffic = df.groupby('ASIN')['æµé‡è´¡çŒ®å€¼'].sum().sort_values(ascending=False).reset_index()

    fig = px.bar(
        asin_traffic, x='ASIN', y='æµé‡è´¡çŒ®å€¼',
        title="å„ASINæµé‡è´¡çŒ®å€¼å¯¹æ¯”",
        labels={'ASIN': 'äº§å“ASIN', 'æµé‡è´¡çŒ®å€¼': 'æµé‡è´¡çŒ®å€¼ (æµé‡å æ¯” * æœˆæœç´¢é‡)'},
        text='æµé‡è´¡çŒ®å€¼'
    )
    fig.update_traces(texttemplate='%{text:,.0f}', textposition='outside')
    fig.update_layout(xaxis_tickangle=-45)
    st.plotly_chart(fig, use_container_width=True)


def display_aggregated_word_frequency(df: pd.DataFrame):
    """ä¸ºåˆå¹¶æ•°æ®å±•ç¤ºå•è¯é¢‘ç‡è¡¨æ ¼å’Œè¯äº‘ã€‚"""
    st.subheader("èšåˆåçš„å•è¯é¢‘ç‡ (Aggregated Word Frequency)")
    word_counts, freq_df = calculate_word_frequency(df)

    if not word_counts:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æµé‡è¯æ¥ç”Ÿæˆå•è¯é¢‘ç‡åˆ†æã€‚")
        return

    st.write("å•è¯é¢‘ç‡ç»Ÿè®¡è¡¨æ ¼")
    st.dataframe(freq_df.style.format({"é¢‘ç‡": "{:.2%}"}), use_container_width=True)

    st.write("å•è¯è¯äº‘")
    wordcloud = WordCloud(
        width=1600, height=800, background_color='white',
        max_words=200, collocations=False
    ).generate_from_frequencies(word_counts)
    fig, ax = plt.subplots(figsize=(16, 8))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)


# --- Streamlit é¡µé¢ä¸»å‡½æ•° ---

def main():
    st.set_page_config(layout="wide", page_title="ASINå…³é”®è¯åˆ†æé¢æ¿")
    st.title("ğŸ“Š ASINåæŸ¥å…³é”®è¯æ•°æ®åˆ†æé¢æ¿")

    uploaded_files = st.file_uploader(
        "è¯·ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªASINåæŸ¥çš„Excelæ–‡ä»¶",
        type="xlsx",
        accept_multiple_files=True
    )

    if not uploaded_files:
        st.info("è¯·ä¸Šä¼ æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")
        return

    # --- å•æ–‡ä»¶å¤„ç†é€»è¾‘ ---
    if len(uploaded_files) == 1:
        uploaded_file = uploaded_files[0]
        result = process_uploaded_file(uploaded_file)
        if result:
            _, df, _ = result
            asin_info = parse_filename(uploaded_file.name)
            asin = asin_info['asin'] if asin_info else "æœªçŸ¥"

            st.success(f"æˆåŠŸåŠ è½½æ–‡ä»¶: **{uploaded_file.name}**")

            # å•æ–‡ä»¶ä»ªè¡¨ç›˜
            display_key_metrics(df, asin=asin) # todoï¼šæ–°å¢ä¸“ç”¨å‡½æ•°
            plot_keyword_traffic(df)

            # --- è¯é¢‘åˆ†æ ---
            st.subheader("ç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡ (Word Frequency)")

            # 1. è®¡ç®—è¯é¢‘ (å…¬å…±é€»è¾‘)
            text = ' '.join(df['æµé‡è¯'].dropna())
            words = re.findall(r'\b\w+\b', text.lower())
            word_counts = Counter(words)
            total_words = sum(word_counts.values())
            freq_df = pd.DataFrame(word_counts.items(), columns=["å•è¯", "å‡ºç°æ¬¡æ•°"])
            freq_df["é¢‘ç‡"] = freq_df["å‡ºç°æ¬¡æ•°"] / total_words
            freq_df = freq_df.sort_values(by="å‡ºç°æ¬¡æ•°", ascending=False)

            # 2. ä¾æ¬¡å±•ç¤ºè¯äº‘å’Œè¡¨æ ¼
            display_word_cloud(word_counts)
            display_frequency_table(freq_df)

            plot_search_volume_and_purchases(df)
            display_raw_data(df)

    # --- å¤šæ–‡ä»¶å¤„ç†é€»è¾‘ ---
    elif len(uploaded_files) > 1:
        individual_sheets = {}
        dfs_for_consolidation = []

        with st.spinner("æ­£åœ¨å¤„ç†å’Œåˆå¹¶æ–‡ä»¶..."):
            for file in uploaded_files:
                result = process_uploaded_file(file)
                if result:
                    sheet_name, original_df, df_for_consolidation = result
                    individual_sheets[sheet_name] = original_df
                    dfs_for_consolidation.append(df_for_consolidation)

        if not dfs_for_consolidation:
            st.error("æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶éƒ½å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
            return

        consolidated_df = pd.concat(dfs_for_consolidation, ignore_index=True)
        st.success(f"æˆåŠŸåˆå¹¶ **{len(dfs_for_consolidation)}** ä¸ªæ–‡ä»¶ï¼")

        # æä¾›åˆå¹¶åçš„æ–‡ä»¶ä¸‹è½½
        excel_bytes = create_excel_file(individual_sheets, consolidated_df)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½åˆå¹¶åçš„Excelæ–‡ä»¶",
            data=excel_bytes,
            file_name="Consolidated_ASIN_Keywords.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        # åˆ›å»ºä¸‹æ‹‰é€‰æ‹©å™¨
        asin_options = ["åˆå¹¶åæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"] + sorted(list(consolidated_df['ASIN'].unique()))
        choice = st.selectbox("è¯·é€‰æ‹©è¦æŸ¥çœ‹çš„è§†å›¾:", asin_options)

        if choice == "åˆå¹¶åæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯":
            display_key_metrics(consolidated_df, is_consolidated=True)
            plot_asin_traffic_contribution(consolidated_df)
            plot_keyword_traffic(consolidated_df)
            display_aggregated_word_frequency(consolidated_df)
            display_raw_data(consolidated_df, title="åˆå¹¶åçš„æ•°æ®è¡¨ (Consolidated Data)")
        else:
            # æ ¹æ®é€‰æ‹©çš„ASINç­›é€‰åŸå§‹æ•°æ®è¿›è¡Œå±•ç¤º
            # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ä»åˆå¹¶å‰çš„æ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”çš„DataFrame
            selected_asin_df = None
            for sheet_name, df in individual_sheets.items():
                if choice in sheet_name:
                    selected_asin_df = df
                    break

            if selected_asin_df is not None:
                display_key_metrics(selected_asin_df, asin=choice)
                plot_keyword_traffic(selected_asin_df)
                display_word_frequency_analysis(selected_asin_df)
                plot_search_volume_and_purchases(selected_asin_df)
                display_raw_data(selected_asin_df)


if __name__ == "__main__":
    main()

