import streamlit as st
import pandas as pd
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re
from collections import Counter
from typing import Optional, Dict, Tuple, List
from io import BytesIO
from shared.sidebar import create_common_sidebar # <-- 1. å¯¼å…¥å‡½æ•°
create_common_sidebar() # <-- 2. è°ƒç”¨å‡½æ•°ï¼Œç¡®ä¿æ¯ä¸ªé¡µé¢éƒ½æœ‰ä¾§è¾¹æ 


# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def parse_filename(filename: str) -> Optional[Dict[str, str]]:
    """
    ä»æ ‡å‡†æ–‡ä»¶åä¸­è§£æå‡ºASINç­‰ä¿¡æ¯ã€‚
    æ–‡ä»¶åæ ¼å¼: ReverseASIN-US-B01N9KSITZ(1584)-20251012.xlsx
    """
    pattern = re.compile(r'ReverseASIN-(.*?)-([A-Z0-9]{10})\(.*\)-\d+')
    match = pattern.match(filename)
    if match:
        return {'country': match.group(1), 'asin': match.group(2)}
    return None


def process_uploaded_file(uploaded_file) -> Optional[Tuple[str, pd.DataFrame, pd.DataFrame]]:
    """
    å¤„ç†å•ä¸ªä¸Šä¼ çš„Excelæ–‡ä»¶ï¼Œæå–ASINå¹¶å‡†å¤‡æ•°æ®ç”¨äºåˆå¹¶ã€‚
    """
    try:
        file_info = parse_filename(uploaded_file.name)
        if not file_info:
            st.warning(f"æ–‡ä»¶å '{uploaded_file.name}' æ ¼å¼ä¸ç¬¦åˆè¦æ±‚ï¼Œå·²è·³è¿‡ã€‚")
            return None

        asin = file_info['asin']
        xls = pd.ExcelFile(uploaded_file, engine='openpyxl')
        sheet_name = xls.sheet_names[0]
        original_df = pd.read_excel(xls, sheet_name=sheet_name)

        # æ•°æ®æ¸…æ´—
        numeric_cols = ['æµé‡å æ¯”', 'è‡ªç„¶æµé‡å æ¯”', 'å¹¿å‘Šæµé‡å æ¯”', 'æœˆæœç´¢é‡', 'è´­ä¹°é‡', 'è´­ä¹°ç‡']
        for col in numeric_cols:
            if col in original_df.columns:
                original_df[col] = pd.to_numeric(original_df[col], errors='coerce').fillna(0)

        df_for_consolidation = original_df.copy()
        df_for_consolidation.insert(0, 'ASIN', asin)
        return sheet_name, original_df, df_for_consolidation
    except Exception as e:
        st.error(f"å¤„ç†æ–‡ä»¶ '{uploaded_file.name}' æ—¶å‡ºé”™: {e}")
        return None


def create_excel_file(individual_sheets: Dict[str, pd.DataFrame], consolidated_df: pd.DataFrame) -> BytesIO:
    """
    åœ¨å†…å­˜ä¸­åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ç‹¬ç«‹sheetå’Œæ€»è¡¨çš„Excelæ–‡ä»¶ã€‚
    """
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        consolidated_df.to_excel(writer, sheet_name='æ€»è¡¨-æ‰€æœ‰ASINæ•´åˆ', index=False)
        for sheet_name, df in individual_sheets.items():
            df.to_excel(writer, sheet_name=sheet_name, index=False)
    output.seek(0)
    return output


# --- æ•°æ®åˆ†æä¸å¯è§†åŒ–å‡½æ•° ---

def display_key_metrics(df: pd.DataFrame, is_consolidated: bool = False, asin: str = ""):
    """å±•ç¤ºå…³é”®æŒ‡æ ‡æ€»è§ˆã€‚"""
    st.subheader("å…³é”®æŒ‡æ ‡æ€»è§ˆ (Key Metrics)")

    total_keywords = df['æµé‡è¯'].nunique()
    total_search_volume = df['æœˆæœç´¢é‡'].sum()
    total_purchases = df['è´­ä¹°é‡'].sum()
    avg_purchase_rate = df['è´­ä¹°ç‡'].mean()

    cols = st.columns(4)
    with cols[0]:
        st.metric("å…³é”®è¯æ€»æ•°", f"{total_keywords:,}")
    with cols[1]:
        st.metric("æœˆæœç´¢æ€»é‡", f"{int(total_search_volume):,}")
    with cols[2]:
        st.metric("æœˆè´­ä¹°æ€»é‡", f"{int(total_purchases):,}")
    with cols[3]:
        st.metric("å¹³å‡è´­ä¹°ç‡", f"{avg_purchase_rate:.2%}")

    if is_consolidated:
        total_asins = df['ASIN'].nunique()
        st.info(f"å½“å‰æ•°æ®ä¸º **{total_asins}** ä¸ªASINçš„åˆå¹¶åˆ†æç»“æœã€‚")
    else:
        st.info(f"å½“å‰æ•°æ®ä¸ºå•ä¸ªASIN **{asin}** çš„åˆ†æç»“æœã€‚")


def plot_keyword_traffic(df: pd.DataFrame):
    """å±•ç¤ºå…³é”®è¯æµé‡çš„å †å æ¡å½¢å›¾ã€‚"""
    st.subheader("å…³é”®è¯æµé‡ (Keyword Traffic)")

    # è¿‡æ»¤æ‰æµé‡å æ¯”ä¸º0çš„æ•°æ®
    df_filtered = df[df['æµé‡å æ¯”'] > 0].copy()
    if df_filtered.empty:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æµé‡æ•°æ®å¯ä¾›å±•ç¤ºã€‚")
        return

    top_20_traffic = df_filtered.sort_values(by="æµé‡å æ¯”", ascending=False).head(20)
    top_20_traffic["è‡ªç„¶æµé‡ç»å¯¹å æ¯”"] = top_20_traffic["æµé‡å æ¯”"] * top_20_traffic["è‡ªç„¶æµé‡å æ¯”"]
    top_20_traffic["å¹¿å‘Šæµé‡ç»å¯¹å æ¯”"] = top_20_traffic["æµé‡å æ¯”"] * top_20_traffic["å¹¿å‘Šæµé‡å æ¯”"]

    plot_df = top_20_traffic.melt(
        id_vars=["æµé‡è¯"],
        value_vars=["è‡ªç„¶æµé‡ç»å¯¹å æ¯”", "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”"],
        var_name="æµé‡ç±»å‹",
        value_name="å æ¯”"
    )

    fig = px.bar(
        plot_df, y="æµé‡è¯", x="å æ¯”", color="æµé‡ç±»å‹", orientation='h',
        title="Top 20 å…³é”®è¯æµé‡åˆ†å¸ƒ",
        labels={"æµé‡è¯": "å…³é”®è¯", "å æ¯”": "æµé‡å æ¯”"},
        color_discrete_map={"è‡ªç„¶æµé‡ç»å¯¹å æ¯”": "#636EFA", "å¹¿å‘Šæµé‡ç»å¯¹å æ¯”": "#EF553B"},
        text="å æ¯”"
    )
    fig.update_traces(texttemplate='%{text:.2%}', textposition='inside', insidetextanchor='middle')
    fig.update_layout(
        xaxis_title="æµé‡å æ¯”", yaxis_title="å…³é”®è¯",
        yaxis={'categoryorder': 'total ascending'}, height=800,
        legend_title_text='æµé‡ç±»å‹',
        xaxis=dict(tickformat=".2%")
    )
    st.plotly_chart(fig, use_container_width=True)


def calculate_word_frequency(df: pd.DataFrame) -> Tuple[Dict[str, int], pd.DataFrame]:
    """ä»æµé‡è¯åˆ—è®¡ç®—å•è¯é¢‘ç‡ã€‚"""
    words = df['æµé‡è¯'].dropna().str.lower().str.split().sum()
    word_counts = Counter(words)
    total_words = sum(word_counts.values())

    freq_df = pd.DataFrame(word_counts.items(), columns=['å•è¯', 'å‡ºç°æ¬¡æ•°'])
    freq_df['é¢‘ç‡'] = freq_df['å‡ºç°æ¬¡æ•°'] / total_words
    freq_df = freq_df.sort_values(by='å‡ºç°æ¬¡æ•°', ascending=False).reset_index(drop=True)

    return word_counts, freq_df


def display_word_frequency_analysis(df: pd.DataFrame):
    """å±•ç¤ºå•è¯é¢‘ç‡è¯äº‘å’Œè¡¨æ ¼ã€‚"""
    st.subheader("ç»„æˆå…³é”®è¯çš„å•è¯é¢‘ç‡ (Word Frequency)")
    word_counts, freq_df = calculate_word_frequency(df)

    if not word_counts:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æµé‡è¯æ¥ç”Ÿæˆå•è¯é¢‘ç‡åˆ†æã€‚")
        return

    col1, col2 = st.columns([2, 1])
    with col1:
        st.write("å•è¯è¯äº‘")
        wordcloud = WordCloud(
            width=800, height=500, background_color='white',
            max_words=150, collocations=False
        ).generate_from_frequencies(word_counts)
        fig, ax = plt.subplots()
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    with col2:
        st.write("å•è¯é¢‘ç‡ç»Ÿè®¡ (Top 20)")
        st.dataframe(
            freq_df.head(20).style.format({"é¢‘ç‡": "{:.2%}"}),
            height=500, use_container_width=True
        )


def plot_search_volume_and_purchases(df: pd.DataFrame):
    """å±•ç¤ºå…³é”®è¯æœç´¢é‡å’Œè´­ä¹°é‡çš„å †å æ¡å½¢å›¾ã€‚"""
    st.subheader("å…³é”®è¯æœç´¢é‡å’Œè´­ä¹°é‡ (Search Volume and Purchases)")

    df_filtered = df[df['æœˆæœç´¢é‡'] > 0].copy()
    if df_filtered.empty:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æœˆæœç´¢é‡æ•°æ®å¯ä¾›å±•ç¤ºã€‚")
        return

    top_20_search = df_filtered.sort_values(by="æœˆæœç´¢é‡", ascending=False).head(20)
    top_20_search["æœªè´­ä¹°é‡"] = top_20_search["æœˆæœç´¢é‡"] - top_20_search["è´­ä¹°é‡"]

    plot_df = top_20_search.melt(
        id_vars=["æµé‡è¯", "è´­ä¹°ç‡"],
        value_vars=["è´­ä¹°é‡", "æœªè´­ä¹°é‡"],
        var_name="ç±»å‹", value_name="æ•°é‡"
    )

    fig = px.bar(
        plot_df, y="æµé‡è¯", x="æ•°é‡", color="ç±»å‹", orientation='h',
        title="Top 20 å…³é”®è¯æœç´¢é‡ä¸è´­ä¹°é‡",
        labels={"æµé‡è¯": "å…³é”®è¯", "æ•°é‡": "æœˆæœç´¢é‡"},
        color_discrete_map={"è´­ä¹°é‡": "#00CC96", "æœªè´­ä¹°é‡": "#FECB52"}
    )

    # ä¸ºæ¯ä¸ªæ¡å½¢æ·»åŠ è´­ä¹°ç‡æ ‡æ³¨
    annotations = []
    for _, row in top_20_search.iterrows():
        annotations.append(dict(
            x=row['æœˆæœç´¢é‡'] * 0.98, y=row['æµé‡è¯'],
            text=f"è´­ä¹°ç‡: {row['è´­ä¹°ç‡']:.2%}",
            showarrow=False, font=dict(color="black", size=10),
            xanchor='right'
        ))
    fig.update_layout(annotations=annotations)

    fig.update_layout(
        xaxis_title="æœˆæœç´¢é‡", yaxis_title="å…³é”®è¯",
        yaxis={'categoryorder': 'total ascending'}, height=800,
        legend_title_text='ç±»å‹'
    )
    st.plotly_chart(fig, use_container_width=True)


def display_raw_data(df: pd.DataFrame, title: str = "åŸå§‹æ•°æ® (Raw Data)"):
    """å±•ç¤ºåŸå§‹æ•°æ®è¡¨æ ¼ã€‚"""
    st.subheader(title)
    st.dataframe(df)


# --- åˆå¹¶æ–‡ä»¶åçš„ä¸“å±å¯è§†åŒ–å‡½æ•° ---

def plot_asin_traffic_contribution(df: pd.DataFrame):
    """å±•ç¤ºå„ASINæµé‡è´¡çŒ®å¯¹æ¯”çš„æŸ±çŠ¶å›¾ã€‚"""
    st.subheader("å„ASINæµé‡è´¡çŒ®å¯¹æ¯” (ASIN Traffic Contribution)")

    # è®¡ç®—æ¯ä¸ªASINçš„æ€»æµé‡è´¡çŒ®å€¼ï¼ˆæµé‡å æ¯” * æœˆæœç´¢é‡ï¼‰
    df['æµé‡è´¡çŒ®å€¼'] = df['æµé‡å æ¯”'] * df['æœˆæœç´¢é‡']
    asin_traffic = df.groupby('ASIN')['æµé‡è´¡çŒ®å€¼'].sum().sort_values(ascending=False).reset_index()

    fig = px.bar(
        asin_traffic, x='ASIN', y='æµé‡è´¡çŒ®å€¼',
        title="å„ASINæµé‡è´¡çŒ®å€¼å¯¹æ¯”",
        labels={'ASIN': 'äº§å“ASIN', 'æµé‡è´¡çŒ®å€¼': 'æµé‡è´¡çŒ®å€¼ (æµé‡å æ¯” * æœˆæœç´¢é‡)'},
        text='æµé‡è´¡çŒ®å€¼'
    )
    fig.update_traces(texttemplate='%{text:,.0f}', textposition='outside')
    fig.update_layout(xaxis_tickangle=-45)
    st.plotly_chart(fig, use_container_width=True)


def display_aggregated_word_frequency(df: pd.DataFrame):
    """ä¸ºåˆå¹¶æ•°æ®å±•ç¤ºå•è¯é¢‘ç‡è¡¨æ ¼å’Œè¯äº‘ã€‚"""
    st.subheader("èšåˆåçš„å•è¯é¢‘ç‡ (Aggregated Word Frequency)")
    word_counts, freq_df = calculate_word_frequency(df)

    if not word_counts:
        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æµé‡è¯æ¥ç”Ÿæˆå•è¯é¢‘ç‡åˆ†æã€‚")
        return

    st.write("å•è¯é¢‘ç‡ç»Ÿè®¡è¡¨æ ¼")
    st.dataframe(freq_df.style.format({"é¢‘ç‡": "{:.2%}"}), use_container_width=True)

    st.write("å•è¯è¯äº‘")
    wordcloud = WordCloud(
        width=1600, height=800, background_color='white',
        max_words=200, collocations=False
    ).generate_from_frequencies(word_counts)
    fig, ax = plt.subplots(figsize=(16, 8))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)


# --- Streamlit é¡µé¢ä¸»å‡½æ•° ---

def main():
    st.set_page_config(layout="wide", page_title="ASINå…³é”®è¯åˆ†æé¢æ¿")
    st.title("ğŸ“Š ASINåæŸ¥å…³é”®è¯æ•°æ®åˆ†æé¢æ¿")

    uploaded_files = st.file_uploader(
        "è¯·ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªASINåæŸ¥çš„Excelæ–‡ä»¶",
        type="xlsx",
        accept_multiple_files=True
    )

    if not uploaded_files:
        st.info("è¯·ä¸Šä¼ æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")
        return

    # --- å•æ–‡ä»¶å¤„ç†é€»è¾‘ ---
    if len(uploaded_files) == 1:
        uploaded_file = uploaded_files[0]
        result = process_uploaded_file(uploaded_file)
        if result:
            _, df, _ = result
            asin_info = parse_filename(uploaded_file.name)
            asin = asin_info['asin'] if asin_info else "æœªçŸ¥"

            st.success(f"æˆåŠŸåŠ è½½æ–‡ä»¶: **{uploaded_file.name}**")

            # å•æ–‡ä»¶ä»ªè¡¨ç›˜
            display_key_metrics(df, asin=asin)
            plot_keyword_traffic(df)
            display_word_frequency_analysis(df)
            plot_search_volume_and_purchases(df)
            display_raw_data(df)

    # --- å¤šæ–‡ä»¶å¤„ç†é€»è¾‘ ---
    elif len(uploaded_files) > 1:
        individual_sheets = {}
        dfs_for_consolidation = []

        with st.spinner("æ­£åœ¨å¤„ç†å’Œåˆå¹¶æ–‡ä»¶..."):
            for file in uploaded_files:
                result = process_uploaded_file(file)
                if result:
                    sheet_name, original_df, df_for_consolidation = result
                    individual_sheets[sheet_name] = original_df
                    dfs_for_consolidation.append(df_for_consolidation)

        if not dfs_for_consolidation:
            st.error("æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶éƒ½å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
            return

        consolidated_df = pd.concat(dfs_for_consolidation, ignore_index=True)
        st.success(f"æˆåŠŸåˆå¹¶ **{len(dfs_for_consolidation)}** ä¸ªæ–‡ä»¶ï¼")

        # æä¾›åˆå¹¶åçš„æ–‡ä»¶ä¸‹è½½
        excel_bytes = create_excel_file(individual_sheets, consolidated_df)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½åˆå¹¶åçš„Excelæ–‡ä»¶",
            data=excel_bytes,
            file_name="Consolidated_ASIN_Keywords.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        # åˆ›å»ºä¸‹æ‹‰é€‰æ‹©å™¨
        asin_options = ["åˆå¹¶åæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"] + sorted(list(consolidated_df['ASIN'].unique()))
        choice = st.selectbox("è¯·é€‰æ‹©è¦æŸ¥çœ‹çš„è§†å›¾:", asin_options)

        if choice == "åˆå¹¶åæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯":
            display_key_metrics(consolidated_df, is_consolidated=True)
            plot_asin_traffic_contribution(consolidated_df)
            plot_keyword_traffic(consolidated_df)
            display_aggregated_word_frequency(consolidated_df)
            display_raw_data(consolidated_df, title="åˆå¹¶åçš„æ•°æ®è¡¨ (Consolidated Data)")
        else:
            # æ ¹æ®é€‰æ‹©çš„ASINç­›é€‰åŸå§‹æ•°æ®è¿›è¡Œå±•ç¤º
            # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ä»åˆå¹¶å‰çš„æ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”çš„DataFrame
            selected_asin_df = None
            for sheet_name, df in individual_sheets.items():
                if choice in sheet_name:
                    selected_asin_df = df
                    break

            if selected_asin_df is not None:
                display_key_metrics(selected_asin_df, asin=choice)
                plot_keyword_traffic(selected_asin_df)
                display_word_frequency_analysis(selected_asin_df)
                plot_search_volume_and_purchases(selected_asin_df)
                display_raw_data(selected_asin_df)


if __name__ == "__main__":
    main()

